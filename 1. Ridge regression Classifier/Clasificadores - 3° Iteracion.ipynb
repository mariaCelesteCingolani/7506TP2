{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7506 - Organizacion de Datos - TP NÂ°2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Librerias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Paquetes Clasicos\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#Paquetes para Preprocesamiento\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import stopwords \n",
    "import unicodedata\n",
    "\n",
    "#Vectorizacion\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#Pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "#Tuning de Parametros\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#Clasificadores\n",
    "from sklearn.linear_model import RidgeClassifier # Classifier using Ridge regression.\n",
    "from sklearn.linear_model import LogisticRegression # Classifier using Logistic Regression.\n",
    "from sklearn.linear_model import Perceptron # Classifier using Perceptron.\n",
    "from sklearn.naive_bayes import MultinomialNB # Classifier using Naive Bayes classifier multinomial\n",
    "from sklearn.svm import LinearSVC #Classifier using Linear Support Vector\n",
    "from sklearn.svm import SVC #C-Support Vector\n",
    "from sklearn.ensemble import GradientBoostingClassifier #Classifier using Gradient Boosting classifier "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset de entranamiento\n",
    "df_train = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset de test\n",
    "df_test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Dataset 'train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>7944</td>\n",
       "      <td>rainstorm</td>\n",
       "      <td>Sicamous, British Columbia</td>\n",
       "      <td>lizzie363 @CstSmith I drove thru deep water in...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7349</th>\n",
       "      <td>10521</td>\n",
       "      <td>wildfire</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reuters Top News: PHOTOS: The Rocky Fire has g...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5199</th>\n",
       "      <td>7424</td>\n",
       "      <td>obliterated</td>\n",
       "      <td>Palmyra, NJ</td>\n",
       "      <td>I was obliterated last night??</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>570</td>\n",
       "      <td>arson</td>\n",
       "      <td>Eldoret, kenya</td>\n",
       "      <td>#Kisii Police in Kisii hunt for students over ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1053</th>\n",
       "      <td>1522</td>\n",
       "      <td>body%20bags</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Womens Handbags Cross Body Geometric Pattern S...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2070</th>\n",
       "      <td>2971</td>\n",
       "      <td>dead</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@GailSimone #IWasDisappointedBy TellTale's The...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7295</th>\n",
       "      <td>10439</td>\n",
       "      <td>wild%20fires</td>\n",
       "      <td>Twitterville</td>\n",
       "      <td>Collection of Articals and Video of West Coast...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5328</th>\n",
       "      <td>7607</td>\n",
       "      <td>pandemonium</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I'll be at SFA very soon....#Pandemonium http:...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>222</td>\n",
       "      <td>airplane%20accident</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pilot Dies In Plane Crash At Car Festival http...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1764</th>\n",
       "      <td>2537</td>\n",
       "      <td>collision</td>\n",
       "      <td>Peterborough, Ontario, Canada</td>\n",
       "      <td>Two-vehicle collision at Fowlers Corners at Hw...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id              keyword                       location  \\\n",
       "5568   7944            rainstorm     Sicamous, British Columbia   \n",
       "7349  10521             wildfire                            NaN   \n",
       "5199   7424          obliterated                    Palmyra, NJ   \n",
       "395     570                arson                 Eldoret, kenya   \n",
       "1053   1522          body%20bags                            NaN   \n",
       "2070   2971                 dead                            NaN   \n",
       "7295  10439         wild%20fires                   Twitterville   \n",
       "5328   7607          pandemonium                            NaN   \n",
       "156     222  airplane%20accident                            NaN   \n",
       "1764   2537            collision  Peterborough, Ontario, Canada   \n",
       "\n",
       "                                                   text  target  \n",
       "5568  lizzie363 @CstSmith I drove thru deep water in...       1  \n",
       "7349  Reuters Top News: PHOTOS: The Rocky Fire has g...       1  \n",
       "5199                     I was obliterated last night??       0  \n",
       "395   #Kisii Police in Kisii hunt for students over ...       1  \n",
       "1053  Womens Handbags Cross Body Geometric Pattern S...       0  \n",
       "2070  @GailSimone #IWasDisappointedBy TellTale's The...       0  \n",
       "7295  Collection of Articals and Video of West Coast...       1  \n",
       "5328  I'll be at SFA very soon....#Pandemonium http:...       1  \n",
       "156   Pilot Dies In Plane Crash At Car Festival http...       1  \n",
       "1764  Two-vehicle collision at Fowlers Corners at Hw...       1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Observo la constitucion general del Dataset de entrenamiento\n",
    "\n",
    "df_train.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7613, 5)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Dimensiones del dataset\n",
    "\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4342\n",
       "1    3271\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cantidad de tweets catalogados como Verdaderos(1) y Falsos(0)\n",
    "\n",
    "df_train['target'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tratamiento de NaN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reemplazo los NaN por el string 'None'\n",
    "\n",
    "df_train.location.fillna('None', inplace=True)\n",
    "df_train.keyword.fillna('None', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1    None     None  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4    None     None             Forest fire near La Ronge Sask. Canada   \n",
       "2   5    None     None  All residents asked to 'shelter in place' are ...   \n",
       "3   6    None     None  13,000 people receive #wildfires evacuation or...   \n",
       "4   7    None     None  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Observo como queda el dataframe\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tweets Repetidos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cuento la cantidad de 'text' que se encuentran repetidos\n",
    "\n",
    "duplicados_train = df_train['text'].duplicated(keep=False)\n",
    "duplicados_train.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Check these out: http://t.co/rOI2NSmEJJ http:/...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Check these out: http://t.co/rOI2NSmEJJ http:/...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>320 [IR] ICEMOON [AFTERSHOCK] | http://t.co/vA...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>320 [IR] ICEMOON [AFTERSHOCK] | http://t.co/vA...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>320 [IR] ICEMOON [AFTERSHOCK] | http://t.co/TH...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7600</th>\n",
       "      <td>Evacuation order lifted for town of Roosevelt:...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7607</th>\n",
       "      <td>#stormchase Violent Record Breaking EF-5 El Re...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>@aria_ahrary @TheTawniest The out of control w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>Police investigating after an e-bike collided ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>179 rows Ã 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  target\n",
       "40    Check these out: http://t.co/rOI2NSmEJJ http:/...       0\n",
       "48    Check these out: http://t.co/rOI2NSmEJJ http:/...       0\n",
       "106   320 [IR] ICEMOON [AFTERSHOCK] | http://t.co/vA...       0\n",
       "115   320 [IR] ICEMOON [AFTERSHOCK] | http://t.co/vA...       0\n",
       "118   320 [IR] ICEMOON [AFTERSHOCK] | http://t.co/TH...       0\n",
       "...                                                 ...     ...\n",
       "7600  Evacuation order lifted for town of Roosevelt:...       1\n",
       "7607  #stormchase Violent Record Breaking EF-5 El Re...       1\n",
       "7609  @aria_ahrary @TheTawniest The out of control w...       1\n",
       "7610  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...       1\n",
       "7611  Police investigating after an e-bike collided ...       1\n",
       "\n",
       "[179 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Elimino las filas donde 'text' esta repetido\n",
    "\n",
    "dup_train = df_train[['text','target']][duplicados_train]\n",
    "dup_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7613, 5)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Verifico que el tamaÃ±o del dataset disminuyo la cantidad de repetidos\n",
    "\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple vista se observa que alguno de los tweets repetidos tienen como target diferentes valores, lo cual en principio es confuso. Se decide eliminar todos los tweets repetidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1    None     None  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4    None     None             Forest fire near La Ronge Sask. Canada   \n",
       "2   5    None     None  All residents asked to 'shelter in place' are ...   \n",
       "3   6    None     None  13,000 people receive #wildfires evacuation or...   \n",
       "4   7    None     None  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Elimino las filas donde 'text' esta repetido\n",
    "\n",
    "df_train = df_train.drop_duplicates('text',keep=False)\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7434, 5)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Verifico que el tamaÃ±o del dataset disminuyo la cantidad de repetidos\n",
    "\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Dataset 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text\n",
       "0   0     NaN      NaN                 Just happened a terrible car crash\n",
       "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
       "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
       "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
       "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Observo la constitucion general del Dataset de test\n",
    "\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3263, 4)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Dimensiones del dataset\n",
    "\n",
    "df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tratamiento de NaN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reemplazo los NaN por el string 'None'\n",
    "\n",
    "df_test.location.fillna('None', inplace=True)\n",
    "df_test.keyword.fillna('None', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text\n",
       "0   0    None     None                 Just happened a terrible car crash\n",
       "1   2    None     None  Heard about #earthquake is different cities, s...\n",
       "2   3    None     None  there is a forest fire at spot pond, geese are...\n",
       "3   9    None     None           Apocalypse lighting. #Spokane #wildfires\n",
       "4  11    None     None      Typhoon Soudelor kills 28 in China and Taiwan"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Observo como queda el dataframe\n",
    "\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tweets Repetidos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicados_test = df_test['text'].duplicated(keep=False)\n",
    "duplicados_test.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>320 [IR] ICEMOON [AFTERSHOCK] | http://t.co/TH...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>320 [IR] ICEMOON [AFTERSHOCK] | http://t.co/TH...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>#AskConnor there's a zombie apocalypse. the it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>To fight bioterrorism sir.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>To fight bioterrorism sir.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>510</th>\n",
       "      <td>70 yrs since the atomic bombing of Hiroshima.....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>540</th>\n",
       "      <td>.@denisleary Not sure how these folks rush int...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>.@denisleary Not sure how these folks rush int...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>607</th>\n",
       "      <td>Bushfire causes first victim in Albania ::  Th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>612</th>\n",
       "      <td>Bushfire causes first victim in Albania ::  Th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1044</th>\n",
       "      <td>@PumpkinMari_Bot lemme just derail this real q...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1051</th>\n",
       "      <td>@PumpkinMari_Bot lemme just derail this real q...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1066</th>\n",
       "      <td>Madhya Pradesh Train Derailment: Village Youth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1072</th>\n",
       "      <td>Madhya Pradesh Train Derailment: Village Youth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1217</th>\n",
       "      <td>IRIN Asia | Red tape tangles Nepal reconstruct...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1233</th>\n",
       "      <td>.POTUS #StrategicPatience is a strategy for #G...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1234</th>\n",
       "      <td>.POTUS #StrategicPatience is a strategy for #G...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1301</th>\n",
       "      <td>Boy saves autistic brother from drowning: A ni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1302</th>\n",
       "      <td>Boy saves autistic brother from drowning: A ni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1481</th>\n",
       "      <td>that exploded &amp;amp; brought about the\\nbeginni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1489</th>\n",
       "      <td>that exploded &amp;amp; brought about the\\nbeginni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1536</th>\n",
       "      <td>11-Year-Old Boy Charged With Manslaughter of T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1537</th>\n",
       "      <td>11-Year-Old Boy Charged With Manslaughter of T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1544</th>\n",
       "      <td>11-Year-Old Boy Charged With Manslaughter of T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1892</th>\n",
       "      <td>Sinjar Massacre Yazidis Blast Lack of Action O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1893</th>\n",
       "      <td>Sinjar Massacre Yazidis Blast Lack of Action O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2043</th>\n",
       "      <td>70 yrs since the atomic bombing of Hiroshima.....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2153</th>\n",
       "      <td>IRIN Asia | Red tape tangles Nepal reconstruct...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2731</th>\n",
       "      <td>International News ÂÃÂ¢Ã¥Ã'Nigeria suicide bomb ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2733</th>\n",
       "      <td>reaad/ plsss Pic of 16yr old PKK suicide bombe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2734</th>\n",
       "      <td>reaad/ plsss Pic of 16yr old PKK suicide bombe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2740</th>\n",
       "      <td>International News ÂÃÂ¢Ã¥Ã'Nigeria suicide bomb ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2741</th>\n",
       "      <td>#Bestnaijamade: 16yr old PKK suicide bomber wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2744</th>\n",
       "      <td>#Bestnaijamade: 16yr old PKK suicide bomber wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2904</th>\n",
       "      <td>GSP issues STRONG THUNDERSTORM WILL IMPACT POR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2910</th>\n",
       "      <td>GSP issues STRONG THUNDERSTORM WILL IMPACT POR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3114</th>\n",
       "      <td>#AskConnor there's a zombie apocalypse. the it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3230</th>\n",
       "      <td>Wreckage 'Conclusively Confirmed' as From MH37...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3235</th>\n",
       "      <td>Wreckage 'Conclusively Confirmed' as From MH37...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text\n",
       "47    320 [IR] ICEMOON [AFTERSHOCK] | http://t.co/TH...\n",
       "53    320 [IR] ICEMOON [AFTERSHOCK] | http://t.co/TH...\n",
       "128   #AskConnor there's a zombie apocalypse. the it...\n",
       "285                          To fight bioterrorism sir.\n",
       "286                          To fight bioterrorism sir.\n",
       "510   70 yrs since the atomic bombing of Hiroshima.....\n",
       "540   .@denisleary Not sure how these folks rush int...\n",
       "600   .@denisleary Not sure how these folks rush int...\n",
       "607   Bushfire causes first victim in Albania ::  Th...\n",
       "612   Bushfire causes first victim in Albania ::  Th...\n",
       "1044  @PumpkinMari_Bot lemme just derail this real q...\n",
       "1051  @PumpkinMari_Bot lemme just derail this real q...\n",
       "1066  Madhya Pradesh Train Derailment: Village Youth...\n",
       "1072  Madhya Pradesh Train Derailment: Village Youth...\n",
       "1217  IRIN Asia | Red tape tangles Nepal reconstruct...\n",
       "1233  .POTUS #StrategicPatience is a strategy for #G...\n",
       "1234  .POTUS #StrategicPatience is a strategy for #G...\n",
       "1301  Boy saves autistic brother from drowning: A ni...\n",
       "1302  Boy saves autistic brother from drowning: A ni...\n",
       "1481  that exploded &amp; brought about the\\nbeginni...\n",
       "1489  that exploded &amp; brought about the\\nbeginni...\n",
       "1536  11-Year-Old Boy Charged With Manslaughter of T...\n",
       "1537  11-Year-Old Boy Charged With Manslaughter of T...\n",
       "1544  11-Year-Old Boy Charged With Manslaughter of T...\n",
       "1892  Sinjar Massacre Yazidis Blast Lack of Action O...\n",
       "1893  Sinjar Massacre Yazidis Blast Lack of Action O...\n",
       "2043  70 yrs since the atomic bombing of Hiroshima.....\n",
       "2153  IRIN Asia | Red tape tangles Nepal reconstruct...\n",
       "2731  International News ÂÃÂ¢Ã¥Ã'Nigeria suicide bomb ...\n",
       "2733  reaad/ plsss Pic of 16yr old PKK suicide bombe...\n",
       "2734  reaad/ plsss Pic of 16yr old PKK suicide bombe...\n",
       "2740  International News ÂÃÂ¢Ã¥Ã'Nigeria suicide bomb ...\n",
       "2741  #Bestnaijamade: 16yr old PKK suicide bomber wh...\n",
       "2744  #Bestnaijamade: 16yr old PKK suicide bomber wh...\n",
       "2904  GSP issues STRONG THUNDERSTORM WILL IMPACT POR...\n",
       "2910  GSP issues STRONG THUNDERSTORM WILL IMPACT POR...\n",
       "3114  #AskConnor there's a zombie apocalypse. the it...\n",
       "3230  Wreckage 'Conclusively Confirmed' as From MH37...\n",
       "3235  Wreckage 'Conclusively Confirmed' as From MH37..."
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dup_test = df_test[['text']][duplicados_test]\n",
    "dup_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso no es necesario eliminar los tweets. Seria conveniente verificar que los mismos fueron catalogados de igual forma al finalizar el analisis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Preprocesamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se definen funciones especificas para realizar el preprocesamiento de los datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformar el texto a minuscula - OK\n",
    "\n",
    "def minuscula(texto):\n",
    "    return texto.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remover URL - OK\n",
    "\n",
    "def remover_url(texto):\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url.sub(r'', texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remover Usuarios (''@usuario') - OK\n",
    "#Si aparecen usuarios combinados ('@usuario__usuario') queda '_usuario' y la funcion remover_no_alfabeto(texto) lo elimina\n",
    "\n",
    "def remover_usuario(texto):\n",
    "    text = re.sub(r\"\\@[A-Za-z0-9]+\", \"\", texto)\n",
    "    return texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remover Emoji - OK\n",
    "\n",
    "def remover_emoji(texto):\n",
    "    emoji_patrones = re.compile(\n",
    "        '['\n",
    "        u'\\U0001F600-\\U0001F64F' \n",
    "        u'\\U0001F300-\\U0001F5FF' \n",
    "        u'\\U0001F680-\\U0001F6FF' \n",
    "        u'\\U0001F1E0-\\U0001F1FF' \n",
    "        u'\\U00002702-\\U000027B0'\n",
    "        u'\\U000024C2-\\U0001F251'\n",
    "        ']+',\n",
    "        flags=re.UNICODE)\n",
    "    return emoji_patrones.sub(r'', texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Expandir las Abreviaturas - OK\n",
    "\n",
    "abreviaturas = {\n",
    "    \"$\" : \" dollar \",\n",
    "    \"â¬\" : \" euro \",\n",
    "    \"4ao\" : \"for adults only\",\n",
    "    \"a.m\" : \"before midday\",\n",
    "    \"a3\" : \"anytime anywhere anyplace\",\n",
    "    \"aamof\" : \"as a matter of fact\",\n",
    "    \"acct\" : \"account\",\n",
    "    \"adih\" : \"another day in hell\",\n",
    "    \"afaic\" : \"as far as i am concerned\",\n",
    "    \"afaict\" : \"as far as i can tell\",\n",
    "    \"afaik\" : \"as far as i know\",\n",
    "    \"afair\" : \"as far as i remember\",\n",
    "    \"afk\" : \"away from keyboard\",\n",
    "    \"app\" : \"application\",\n",
    "    \"approx\" : \"approximately\",\n",
    "    \"apps\" : \"applications\",\n",
    "    \"asap\" : \"as soon as possible\",\n",
    "    \"asl\" : \"age, sex, location\",\n",
    "    \"atk\" : \"at the keyboard\",\n",
    "    \"ave.\" : \"avenue\",\n",
    "    \"aymm\" : \"are you my mother\",\n",
    "    \"ayor\" : \"at your own risk\", \n",
    "    \"b&b\" : \"bed and breakfast\",\n",
    "    \"b+b\" : \"bed and breakfast\",\n",
    "    \"b.c\" : \"before christ\",\n",
    "    \"b2b\" : \"business to business\",\n",
    "    \"b2c\" : \"business to customer\",\n",
    "    \"b4\" : \"before\",\n",
    "    \"b4n\" : \"bye for now\",\n",
    "    \"b@u\" : \"back at you\",\n",
    "    \"bae\" : \"before anyone else\",\n",
    "    \"bak\" : \"back at keyboard\",\n",
    "    \"bbbg\" : \"bye bye be good\",\n",
    "    \"bbc\" : \"british broadcasting corporation\",\n",
    "    \"bbias\" : \"be back in a second\",\n",
    "    \"bbl\" : \"be back later\",\n",
    "    \"bbs\" : \"be back soon\",\n",
    "    \"be4\" : \"before\",\n",
    "    \"bfn\" : \"bye for now\",\n",
    "    \"blvd\" : \"boulevard\",\n",
    "    \"bout\" : \"about\",\n",
    "    \"brb\" : \"be right back\",\n",
    "    \"bros\" : \"brothers\",\n",
    "    \"brt\" : \"be right there\",\n",
    "    \"bsaaw\" : \"big smile and a wink\",\n",
    "    \"btw\" : \"by the way\",\n",
    "    \"bwl\" : \"bursting with laughter\",\n",
    "    \"c/o\" : \"care of\",\n",
    "    \"cet\" : \"central european time\",\n",
    "    \"cf\" : \"compare\",\n",
    "    \"cia\" : \"central intelligence agency\",\n",
    "    \"csl\" : \"can not stop laughing\",\n",
    "    \"cu\" : \"see you\",\n",
    "    \"cul8r\" : \"see you later\",\n",
    "    \"cv\" : \"curriculum vitae\",\n",
    "    \"cwot\" : \"complete waste of time\",\n",
    "    \"cya\" : \"see you\",\n",
    "    \"cyt\" : \"see you tomorrow\",\n",
    "    \"dae\" : \"does anyone else\",\n",
    "    \"dbmib\" : \"do not bother me i am busy\",\n",
    "    \"diy\" : \"do it yourself\",\n",
    "    \"dm\" : \"direct message\",\n",
    "    \"dwh\" : \"during work hours\",\n",
    "    \"e123\" : \"easy as one two three\",\n",
    "    \"eet\" : \"eastern european time\",\n",
    "    \"eg\" : \"example\",\n",
    "    \"embm\" : \"early morning business meeting\",\n",
    "    \"encl\" : \"enclosed\",\n",
    "    \"encl.\" : \"enclosed\",\n",
    "    \"etc\" : \"and so on\",\n",
    "    \"faq\" : \"frequently asked questions\",\n",
    "    \"fawc\" : \"for anyone who cares\",\n",
    "    \"fb\" : \"facebook\",\n",
    "    \"fc\" : \"fingers crossed\",\n",
    "    \"fig\" : \"figure\",\n",
    "    \"fimh\" : \"forever in my heart\", \n",
    "    \"ft.\" : \"feet\",\n",
    "    \"ft\" : \"featuring\",\n",
    "    \"ftl\" : \"for the loss\",\n",
    "    \"ftw\" : \"for the win\",\n",
    "    \"fwiw\" : \"for what it is worth\",\n",
    "    \"fyi\" : \"for your information\",\n",
    "    \"g9\" : \"genius\",\n",
    "    \"gahoy\" : \"get a hold of yourself\",\n",
    "    \"gal\" : \"get a life\",\n",
    "    \"gcse\" : \"general certificate of secondary education\",\n",
    "    \"gfn\" : \"gone for now\",\n",
    "    \"gg\" : \"good game\",\n",
    "    \"gl\" : \"good luck\",\n",
    "    \"glhf\" : \"good luck have fun\",\n",
    "    \"gmt\" : \"greenwich mean time\",\n",
    "    \"gmta\" : \"great minds think alike\",\n",
    "    \"gn\" : \"good night\",\n",
    "    \"g.o.a.t\" : \"greatest of all time\",\n",
    "    \"goat\" : \"greatest of all time\",\n",
    "    \"goi\" : \"get over it\",\n",
    "    \"gps\" : \"global positioning system\",\n",
    "    \"gr8\" : \"great\",\n",
    "    \"gratz\" : \"congratulations\",\n",
    "    \"gyal\" : \"girl\",\n",
    "    \"h&c\" : \"hot and cold\",\n",
    "    \"hp\" : \"horsepower\",\n",
    "    \"hr\" : \"hour\",\n",
    "    \"hrh\" : \"his royal highness\",\n",
    "    \"ht\" : \"height\",\n",
    "    \"ibrb\" : \"i will be right back\",\n",
    "    \"ic\" : \"i see\",\n",
    "    \"icq\" : \"i seek you\",\n",
    "    \"icymi\" : \"in case you missed it\",\n",
    "    \"idc\" : \"i do not care\",\n",
    "    \"idgadf\" : \"i do not give a damn fuck\",\n",
    "    \"idgaf\" : \"i do not give a fuck\",\n",
    "    \"idk\" : \"i do not know\",\n",
    "    \"ie\" : \"that is\",\n",
    "    \"i.e\" : \"that is\",\n",
    "    \"ifyp\" : \"i feel your pain\",\n",
    "    \"IG\" : \"instagram\",\n",
    "    \"iirc\" : \"if i remember correctly\",\n",
    "    \"ilu\" : \"i love you\",\n",
    "    \"ily\" : \"i love you\",\n",
    "    \"imho\" : \"in my humble opinion\",\n",
    "    \"imo\" : \"in my opinion\",\n",
    "    \"imu\" : \"i miss you\",\n",
    "    \"iow\" : \"in other words\",\n",
    "    \"irl\" : \"in real life\",\n",
    "    \"j4f\" : \"just for fun\",\n",
    "    \"jic\" : \"just in case\",\n",
    "    \"jk\" : \"just kidding\",\n",
    "    \"jsyk\" : \"just so you know\",\n",
    "    \"l8r\" : \"later\",\n",
    "    \"lb\" : \"pound\",\n",
    "    \"lbs\" : \"pounds\",\n",
    "    \"ldr\" : \"long distance relationship\",\n",
    "    \"lmao\" : \"laugh my ass off\",\n",
    "    \"lmfao\" : \"laugh my fucking ass off\",\n",
    "    \"lol\" : \"laughing out loud\",\n",
    "    \"ltd\" : \"limited\",\n",
    "    \"ltns\" : \"long time no see\",\n",
    "    \"m8\" : \"mate\",\n",
    "    \"mf\" : \"motherfucker\",\n",
    "    \"mfs\" : \"motherfuckers\",\n",
    "    \"mfw\" : \"my face when\",\n",
    "    \"mofo\" : \"motherfucker\",\n",
    "    \"mph\" : \"miles per hour\",\n",
    "    \"mr\" : \"mister\",\n",
    "    \"mrw\" : \"my reaction when\",\n",
    "    \"ms\" : \"miss\",\n",
    "    \"mte\" : \"my thoughts exactly\",\n",
    "    \"nagi\" : \"not a good idea\",\n",
    "    \"nbc\" : \"national broadcasting company\",\n",
    "    \"nbd\" : \"not big deal\",\n",
    "    \"nfs\" : \"not for sale\",\n",
    "    \"ngl\" : \"not going to lie\",\n",
    "    \"nhs\" : \"national health service\",\n",
    "    \"nrn\" : \"no reply necessary\",\n",
    "    \"nsfl\" : \"not safe for life\",\n",
    "    \"nsfw\" : \"not safe for work\",\n",
    "    \"nth\" : \"nice to have\",\n",
    "    \"nvr\" : \"never\",\n",
    "    \"nyc\" : \"new york city\",\n",
    "    \"oc\" : \"original content\",\n",
    "    \"og\" : \"original\",\n",
    "    \"ohp\" : \"overhead projector\",\n",
    "    \"oic\" : \"oh i see\",\n",
    "    \"omdb\" : \"over my dead body\",\n",
    "    \"omg\" : \"oh my god\",\n",
    "    \"omw\" : \"on my way\",\n",
    "    \"p.a\" : \"per annum\",\n",
    "    \"p.m\" : \"after midday\",\n",
    "    \"pm\" : \"prime minister\",\n",
    "    \"poc\" : \"people of color\",\n",
    "    \"pov\" : \"point of view\",\n",
    "    \"pp\" : \"pages\",\n",
    "    \"ppl\" : \"people\",\n",
    "    \"prw\" : \"parents are watching\",\n",
    "    \"ps\" : \"postscript\",\n",
    "    \"pt\" : \"point\",\n",
    "    \"ptb\" : \"please text back\",\n",
    "    \"pto\" : \"please turn over\",\n",
    "    \"qpsa\" : \"what happens\", \n",
    "    \"ratchet\" : \"rude\",\n",
    "    \"rbtl\" : \"read between the lines\",\n",
    "    \"rlrt\" : \"real life retweet\", \n",
    "    \"rofl\" : \"rolling on the floor laughing\",\n",
    "    \"roflol\" : \"rolling on the floor laughing out loud\",\n",
    "    \"rotflmao\" : \"rolling on the floor laughing my ass off\",\n",
    "    \"rt\" : \"retweet\",\n",
    "    \"ruok\" : \"are you ok\",\n",
    "    \"sfw\" : \"safe for work\",\n",
    "    \"sk8\" : \"skate\",\n",
    "    \"smh\" : \"shake my head\",\n",
    "    \"sq\" : \"square\",\n",
    "    \"srsly\" : \"seriously\", \n",
    "    \"ssdd\" : \"same stuff different day\",\n",
    "    \"tbh\" : \"to be honest\",\n",
    "    \"tbs\" : \"tablespooful\",\n",
    "    \"tbsp\" : \"tablespooful\",\n",
    "    \"tfw\" : \"that feeling when\",\n",
    "    \"thks\" : \"thank you\",\n",
    "    \"tho\" : \"though\",\n",
    "    \"thx\" : \"thank you\",\n",
    "    \"tia\" : \"thanks in advance\",\n",
    "    \"til\" : \"today i learned\",\n",
    "    \"tl;dr\" : \"too long i did not read\",\n",
    "    \"tldr\" : \"too long i did not read\",\n",
    "    \"tmb\" : \"tweet me back\",\n",
    "    \"tntl\" : \"trying not to laugh\",\n",
    "    \"ttyl\" : \"talk to you later\",\n",
    "    \"u\" : \"you\",\n",
    "    \"u2\" : \"you too\",\n",
    "    \"u4e\" : \"yours for ever\",\n",
    "    \"utc\" : \"coordinated universal time\",\n",
    "    \"w/\" : \"with\",\n",
    "    \"w/o\" : \"without\",\n",
    "    \"w8\" : \"wait\",\n",
    "    \"wassup\" : \"what is up\",\n",
    "    \"wb\" : \"welcome back\",\n",
    "    \"wtf\" : \"what the fuck\",\n",
    "    \"wtg\" : \"way to go\",\n",
    "    \"wtpa\" : \"where the party at\",\n",
    "    \"wuf\" : \"where are you from\",\n",
    "    \"wuzup\" : \"what is up\",\n",
    "    \"wywh\" : \"wish you were here\",\n",
    "    \"yd\" : \"yard\",\n",
    "    \"ygtr\" : \"you got that right\",\n",
    "    \"ynk\" : \"you never know\",\n",
    "    \"zzz\" : \"sleeping bored and tired\"\n",
    "}\n",
    "\n",
    "def expandir_abreviatura(texto,mapping = abreviaturas):\n",
    "    texto = ' '.join([mapping[t] if t in mapping else t for t in texto.split(\" \")])\n",
    "    return texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Expandir las Contracciones - OK\n",
    "\n",
    "contracciones_mapeo = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \n",
    "                       \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \n",
    "                       \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \n",
    "                       \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \n",
    "                       \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\n",
    "                       \"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \n",
    "                       \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \n",
    "                       \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\",\n",
    "                       \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \n",
    "                       \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \n",
    "                       \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \n",
    "                       \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \n",
    "                       \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \n",
    "                       \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \n",
    "                       \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \n",
    "                       \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \n",
    "                       \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" }\n",
    "\n",
    "def expandir_contraccion(texto,mapping = contracciones_mapeo):\n",
    "    specials =[\"â\", \"â\", \"Â´\", \"`\"]\n",
    "    for s in specials:\n",
    "        texto = texto.replace(s,\"'\")\n",
    "    \n",
    "    texto = ' '.join([mapping[t] if t in mapping else t for t in texto.split(\" \")])\n",
    "    return texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remover los tags HTML - OK\n",
    "\n",
    "def remover_tag_html(texto):\n",
    "    html = re.compile(r'<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n",
    "    return re.sub(html, '', texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remover acentos - OK\n",
    "\n",
    "def remover_acento(texto):\n",
    "    texto = unicodedata.normalize('NFKD', texto).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remover Puntos - OK\n",
    "\n",
    "def remover_punto(texto):\n",
    "    import string\n",
    "    texto = ''.join([c for c in texto if c not in string.punctuation])\n",
    "    return texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remover Numeros - OK\n",
    "\n",
    "def remover_numero(texto):\n",
    "    texto = ''.join([i for i in texto if not i.isdigit()])\n",
    "    return texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remover espacios en Blanco (extras/tabs) - OK\n",
    "\n",
    "def remover_espacio_extra(texto):\n",
    "    import re\n",
    "    pattern = r'^\\s*|\\s\\s*'\n",
    "    return re.sub(pattern, ' ', texto).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remueve todo lo que se sea del alfabeto - OK\n",
    "\n",
    "def remover_no_alfabeto(texto):\n",
    "    return ' '.join([i for i in texto.split() if i.isalpha() == True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remover Stop-Word - OK\n",
    "\n",
    "def remover_stop_word(texto):\n",
    "    return \" \".join ([word for word in word_tokenize(texto) if not word in stopwords.words('english')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lematizar - OK\n",
    "\n",
    "def lematizar(texto):\n",
    "    lemma = WordNetLemmatizer()\n",
    "    return \" \".join([lemma.lemmatize(word) for word in word_tokenize(texto)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez definidias todas las funciones que se utilizaran para realizar la limpieza se define una funcion de ejecuta todas estas a una columna del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocesar_df(df, col_name, clean_col_name):\n",
    "    df[clean_col_name] = df[col_name].apply(lambda x: minuscula (x))\\\n",
    "                                    .apply(lambda x: remover_url(x))\\\n",
    "                                    .apply(lambda x: remover_usuario(x))\\\n",
    "                                    .apply(lambda x: remover_emoji(x))\\\n",
    "                                    .apply(lambda x: expandir_abreviatura(x))\\\n",
    "                                    .apply(lambda x: expandir_contraccion(x))\\\n",
    "                                    .apply(lambda x: remover_tag_html(x))\\\n",
    "                                    .apply(lambda x: remover_acento(x))\\\n",
    "                                    .apply(lambda x: remover_punto(x))\\\n",
    "                                    .apply(lambda x: remover_numero(x))\\\n",
    "                                    .apply(lambda x: remover_espacio_extra(x))\\\n",
    "                                    .apply(lambda x: remover_no_alfabeto(x))\\\n",
    "                                    .apply(lambda x: remover_stop_word(x))\\\n",
    "                                    .apply(lambda x: lematizar(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Preprocesamiento del Dataset 'train' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizo el preprocesamiento de la comlumna **'text'** del dataset **'train'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Llamo a la funcion que realiza el preprocesamiento\n",
    "\n",
    "preprocesar_df(df_train,'text', 'texto_preprocesado')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>texto_preprocesado</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>deed reason earthquake may allah forgive u</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>resident asked shelter place notified officer ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>people receive wildfire evacuation order calif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>got sent photo ruby alaska smoke wildfire pour...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>#RockyFire Update =&gt; California Hwy. 20 closed...</td>\n",
       "      <td>1</td>\n",
       "      <td>rockyfire update california hwy closed directi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>#flood #disaster Heavy rain causes flash flood...</td>\n",
       "      <td>1</td>\n",
       "      <td>flood disaster heavy rain cause flash flooding...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>13</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>I'm on top of the hill and I can see a fire in...</td>\n",
       "      <td>1</td>\n",
       "      <td>top hill see fire wood</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>14</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>There's an emergency evacuation happening now ...</td>\n",
       "      <td>1</td>\n",
       "      <td>emergency evacuation happening building across...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>15</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>I'm afraid that the tornado is coming to our a...</td>\n",
       "      <td>1</td>\n",
       "      <td>afraid tornado coming area</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>16</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Three people died from the heat wave so far</td>\n",
       "      <td>1</td>\n",
       "      <td>three people died heat wave far</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>17</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Haha South Tampa is getting flooded hah- WAIT ...</td>\n",
       "      <td>1</td>\n",
       "      <td>haha south tampa getting flooded hah wait seco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>18</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>#raining #flooding #Florida #TampaBay #Tampa 1...</td>\n",
       "      <td>1</td>\n",
       "      <td>raining flooding florida tampabay tampa day lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>19</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>#Flood in Bago Myanmar #We arrived Bago</td>\n",
       "      <td>1</td>\n",
       "      <td>flood bago myanmar arrived bago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>20</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Damage to school bus on 80 in multi car crash ...</td>\n",
       "      <td>1</td>\n",
       "      <td>damage school bus multi car crash breaking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>23</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>What's up man?</td>\n",
       "      <td>0</td>\n",
       "      <td>man</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>24</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>I love fruits</td>\n",
       "      <td>0</td>\n",
       "      <td>love fruit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>25</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Summer is lovely</td>\n",
       "      <td>0</td>\n",
       "      <td>summer lovely</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>26</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>My car is so fast</td>\n",
       "      <td>0</td>\n",
       "      <td>car fast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>28</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>What a goooooooaaaaaal!!!!!!</td>\n",
       "      <td>0</td>\n",
       "      <td>goooooooaaaaaal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>31</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>this is ridiculous....</td>\n",
       "      <td>0</td>\n",
       "      <td>ridiculous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>32</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>London is cool ;)</td>\n",
       "      <td>0</td>\n",
       "      <td>london cool</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>33</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Love skiing</td>\n",
       "      <td>0</td>\n",
       "      <td>love skiing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>34</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>What a wonderful day!</td>\n",
       "      <td>0</td>\n",
       "      <td>wonderful day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>36</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>LOOOOOOL</td>\n",
       "      <td>0</td>\n",
       "      <td>looooool</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>37</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>No way...I can't eat that shit</td>\n",
       "      <td>0</td>\n",
       "      <td>wayi eat shit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>38</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Was in NYC last week!</td>\n",
       "      <td>0</td>\n",
       "      <td>new york city last week</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>39</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Love my girlfriend</td>\n",
       "      <td>0</td>\n",
       "      <td>love girlfriend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>40</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Cooool :)</td>\n",
       "      <td>0</td>\n",
       "      <td>cooool</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>41</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Do you like pasta?</td>\n",
       "      <td>0</td>\n",
       "      <td>like pasta</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id keyword location                                               text  \\\n",
       "0    1    None     None  Our Deeds are the Reason of this #earthquake M...   \n",
       "1    4    None     None             Forest fire near La Ronge Sask. Canada   \n",
       "2    5    None     None  All residents asked to 'shelter in place' are ...   \n",
       "3    6    None     None  13,000 people receive #wildfires evacuation or...   \n",
       "4    7    None     None  Just got sent this photo from Ruby #Alaska as ...   \n",
       "5    8    None     None  #RockyFire Update => California Hwy. 20 closed...   \n",
       "6   10    None     None  #flood #disaster Heavy rain causes flash flood...   \n",
       "7   13    None     None  I'm on top of the hill and I can see a fire in...   \n",
       "8   14    None     None  There's an emergency evacuation happening now ...   \n",
       "9   15    None     None  I'm afraid that the tornado is coming to our a...   \n",
       "10  16    None     None        Three people died from the heat wave so far   \n",
       "11  17    None     None  Haha South Tampa is getting flooded hah- WAIT ...   \n",
       "12  18    None     None  #raining #flooding #Florida #TampaBay #Tampa 1...   \n",
       "13  19    None     None            #Flood in Bago Myanmar #We arrived Bago   \n",
       "14  20    None     None  Damage to school bus on 80 in multi car crash ...   \n",
       "15  23    None     None                                     What's up man?   \n",
       "16  24    None     None                                      I love fruits   \n",
       "17  25    None     None                                   Summer is lovely   \n",
       "18  26    None     None                                  My car is so fast   \n",
       "19  28    None     None                       What a goooooooaaaaaal!!!!!!   \n",
       "20  31    None     None                             this is ridiculous....   \n",
       "21  32    None     None                                  London is cool ;)   \n",
       "22  33    None     None                                        Love skiing   \n",
       "23  34    None     None                              What a wonderful day!   \n",
       "24  36    None     None                                           LOOOOOOL   \n",
       "25  37    None     None                     No way...I can't eat that shit   \n",
       "26  38    None     None                              Was in NYC last week!   \n",
       "27  39    None     None                                 Love my girlfriend   \n",
       "28  40    None     None                                          Cooool :)   \n",
       "29  41    None     None                                 Do you like pasta?   \n",
       "\n",
       "    target                                 texto_preprocesado  \n",
       "0        1         deed reason earthquake may allah forgive u  \n",
       "1        1              forest fire near la ronge sask canada  \n",
       "2        1  resident asked shelter place notified officer ...  \n",
       "3        1  people receive wildfire evacuation order calif...  \n",
       "4        1  got sent photo ruby alaska smoke wildfire pour...  \n",
       "5        1  rockyfire update california hwy closed directi...  \n",
       "6        1  flood disaster heavy rain cause flash flooding...  \n",
       "7        1                             top hill see fire wood  \n",
       "8        1  emergency evacuation happening building across...  \n",
       "9        1                         afraid tornado coming area  \n",
       "10       1                    three people died heat wave far  \n",
       "11       1  haha south tampa getting flooded hah wait seco...  \n",
       "12       1  raining flooding florida tampabay tampa day lo...  \n",
       "13       1                    flood bago myanmar arrived bago  \n",
       "14       1         damage school bus multi car crash breaking  \n",
       "15       0                                                man  \n",
       "16       0                                         love fruit  \n",
       "17       0                                      summer lovely  \n",
       "18       0                                           car fast  \n",
       "19       0                                    goooooooaaaaaal  \n",
       "20       0                                         ridiculous  \n",
       "21       0                                        london cool  \n",
       "22       0                                        love skiing  \n",
       "23       0                                      wonderful day  \n",
       "24       0                                           looooool  \n",
       "25       0                                      wayi eat shit  \n",
       "26       0                            new york city last week  \n",
       "27       0                                    love girlfriend  \n",
       "28       0                                             cooool  \n",
       "29       0                                         like pasta  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Observo las 5 primeras filas\n",
    "\n",
    "df_train.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7434, 6)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Dimensiones del dataset\n",
    "\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defino los vectores que se utilizaran para el entrenamiento de los modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_original = df_train['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_preprocesado = df_train['texto_preprocesado']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = df_train['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Preprocesamiento del Dataset 'test' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizo el preprocesamiento de la comlumna **'text'** del dataset **'test'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocesar_df(df_test,'text', 'texto_preprocesado')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>texto_preprocesado</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "      <td>happened terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "      <td>heard earthquake different city stay safe ever...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "      <td>forest fire spot pond goose fleeing across str...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "      <td>apocalypse lighting spokane wildfire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "      <td>typhoon soudelor kill china taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   0    None     None                 Just happened a terrible car crash   \n",
       "1   2    None     None  Heard about #earthquake is different cities, s...   \n",
       "2   3    None     None  there is a forest fire at spot pond, geese are...   \n",
       "3   9    None     None           Apocalypse lighting. #Spokane #wildfires   \n",
       "4  11    None     None      Typhoon Soudelor kills 28 in China and Taiwan   \n",
       "\n",
       "                                  texto_preprocesado  \n",
       "0                        happened terrible car crash  \n",
       "1  heard earthquake different city stay safe ever...  \n",
       "2  forest fire spot pond goose fleeing across str...  \n",
       "3               apocalypse lighting spokane wildfire  \n",
       "4                 typhoon soudelor kill china taiwan  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Observo las 5 primeras filas\n",
    "\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3263, 5)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defino los vectores que se utilizaran para el entrenamiento de los modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_original = df_test['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_preprocesado = df_test['texto_preprocesado']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Hiperparametros de Vectorizadores\n",
    "\n",
    "Se definen los Hiperparametros que se utilizaran en GridSearchCV para realizar el tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parametros para el vectorizador BOW - CountVectorizer()\n",
    "\n",
    "parametros_bow = {'vectorizador__strip_accents':'unicode',\n",
    "                  'vectorizador__ngram_range': [(1,1),(2,2),(1,2)],\n",
    "                  'vectorizador__max_df':[10,20,30],\n",
    "                  'vectorizador__binary':[False,True]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parametros para el vectorizador TFIDF - TfidfVectorizer()\n",
    "\n",
    "parametros_tfidf = {'vectorizador__strip_accents':'unicode',\n",
    "                  'vectorizador__ngram_range': [(1,1),(2,2),(1,2)],\n",
    "                  'vectorizador__max_df':[10,20,30],\n",
    "                  'vectorizador__binary':[False,True],\n",
    "                  'vectorizador__use_idf':[True,False]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Tuning de Parametros (GridSearch) y Resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Ridge Regresion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.1 BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline\n",
    "\n",
    "pipeline_bow_RR = Pipeline([('vectorizador', CountVectorizer()),('clf', RidgeClassifier())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Datos sin Preprocesar**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 468 candidates, totalling 2340 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   11.2s\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:   39.0s\n",
      "[Parallel(n_jobs=-1)]: Done 357 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 640 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1005 tasks      | elapsed:  3.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1450 tasks      | elapsed:  4.8min\n",
      "[Parallel(n_jobs=-1)]: Done 1977 tasks      | elapsed:  6.5min\n",
      "[Parallel(n_jobs=-1)]: Done 2340 out of 2340 | elapsed:  7.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor Score:  0.7152371246179314\n",
      "Mejores Parametros:  {'clf__alpha': 2, 'clf__normalize': True, 'clf__random_state': 42, 'vectorizador__binary': True, 'vectorizador__min_df': 10, 'vectorizador__ngram_range': (1, 2), 'vectorizador__strip_accents': 'unicode'}\n"
     ]
    }
   ],
   "source": [
    "#Dataset sin preprocesamiento\n",
    "\n",
    "#Hiperparametros del vectorizador y del clasificador\n",
    "hiperparametros_1 = {'vectorizador__strip_accents':['unicode'],\n",
    "                     'vectorizador__ngram_range': [(1,1),(2,2),(1,2)],\n",
    "                     'vectorizador__min_df':[10,20,30],\n",
    "                     'vectorizador__binary':[False,True],\n",
    "                     'clf__alpha': [0.001,0.01,0.1,0.5,1,1.5,2,2.5,3,3.5,4,4.5,5],\n",
    "                     'clf__normalize':[False,True],\n",
    "                     'clf__random_state':[42]}\n",
    "\n",
    "#Tuning de Hiperparametros\n",
    "clf_RR_bow_original = GridSearchCV(pipeline_bow_RR, hiperparametros_1,cv=5, n_jobs=-1,verbose=2)\n",
    "clf_RR_bow_original.fit(x_train_original, y_train)\n",
    "\n",
    "\n",
    "#Muestro los resultados\n",
    "\n",
    "print(\"Mejor Score: \", clf_RR_bow_original.best_score_)\n",
    "\n",
    "print(\"Mejores Parametros: \", clf_RR_bow_original.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generacion del SUBMIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leo el archivo .csv modelo que tenemos se utilizar para realizar el submit a Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creo una nueva columna con los valores que predice el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['target'] = clf_RR_bow_original.predict(x_test_original)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardo el archivo .csv para realizar el submit a Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"original_bow_hiper3_ridge_regression.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Datos Preprocesados**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 468 candidates, totalling 2340 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:    7.8s\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:   24.4s\n",
      "[Parallel(n_jobs=-1)]: Done 357 tasks      | elapsed:   53.2s\n",
      "[Parallel(n_jobs=-1)]: Done 640 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1005 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1450 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1977 tasks      | elapsed:  4.1min\n",
      "[Parallel(n_jobs=-1)]: Done 2340 out of 2340 | elapsed:  4.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor Score:  0.6830867971047417\n",
      "Mejores Parametros:  {'clf__alpha': 2, 'clf__normalize': True, 'clf__random_state': 42, 'vectorizador__binary': True, 'vectorizador__min_df': 10, 'vectorizador__ngram_range': (1, 1), 'vectorizador__strip_accents': 'unicode'}\n"
     ]
    }
   ],
   "source": [
    "#Dataset Preprocesado\n",
    "\n",
    "#Hiperparametros del vectorizador y del clasificador\n",
    "hiperparametros_2 = {'vectorizador__strip_accents':['unicode'],\n",
    "                     'vectorizador__ngram_range': [(1,1),(2,2),(1,2)],\n",
    "                     'vectorizador__min_df':[10,20,30],\n",
    "                     'vectorizador__binary':[False,True],\n",
    "                     'clf__alpha': [0.001,0.01,0.1,0.5,1,1.5,2,2.5,3,3.5,4,4.5,5],\n",
    "                     'clf__normalize':[False,True],\n",
    "                     'clf__random_state':[42]}\n",
    "\n",
    "#Tuning de Hiperparametros\n",
    "clf_RR_bow_preprocesado = GridSearchCV(pipeline_bow_RR, hiperparametros_2,cv=5, n_jobs=-1,verbose=2)\n",
    "clf_RR_bow_preprocesado.fit(x_train_preprocesado, y_train)\n",
    "\n",
    "\n",
    "#Muestro los resultados\n",
    "\n",
    "print(\"Mejor Score: \", clf_RR_bow_preprocesado.best_score_)\n",
    "\n",
    "print(\"Mejores Parametros: \", clf_RR_bow_preprocesado.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generacion del SUBMIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leo el archivo .csv modelo que tenemos se utilizar para realizar el submit a Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creo una nueva columna con los valores que predice el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['target'] = clf_RR_bow_preprocesado.predict(x_test_preprocesado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardo el archivo .csv para realizar el submit a Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"preprocesado_bow_hiper3_ridge_regression.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.2 TFIDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline\n",
    "\n",
    "pipeline_tfidf_RR = Pipeline([('vectorizador', TfidfVectorizer()),('clf', RidgeClassifier())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Datos sin Preprocesar**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 936 candidates, totalling 4680 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   14.9s\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:   58.7s\n",
      "[Parallel(n_jobs=-1)]: Done 357 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 640 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1005 tasks      | elapsed:  5.0min\n",
      "[Parallel(n_jobs=-1)]: Done 1450 tasks      | elapsed:  6.8min\n",
      "[Parallel(n_jobs=-1)]: Done 1977 tasks      | elapsed:  8.9min\n",
      "[Parallel(n_jobs=-1)]: Done 2584 tasks      | elapsed: 11.4min\n",
      "[Parallel(n_jobs=-1)]: Done 3273 tasks      | elapsed: 13.9min\n",
      "[Parallel(n_jobs=-1)]: Done 4042 tasks      | elapsed: 16.6min\n",
      "[Parallel(n_jobs=-1)]: Done 4680 out of 4680 | elapsed: 19.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor Score:  0.7097176878845011\n",
      "Mejores Parametros:  {'clf__alpha': 4, 'clf__normalize': True, 'clf__random_state': 42, 'vectorizador__binary': False, 'vectorizador__max_df': 30, 'vectorizador__ngram_range': (1, 2), 'vectorizador__strip_accents': 'unicode', 'vectorizador__use_idf': True}\n"
     ]
    }
   ],
   "source": [
    "#Dataset sin preprocesamiento\n",
    "\n",
    "#Hiperparametros del vectorizador y del clasificador\n",
    "hiperparametros_3 = {'vectorizador__strip_accents':['unicode'],\n",
    "                     'vectorizador__ngram_range': [(1,1),(2,2),(1,2)],\n",
    "                     'vectorizador__max_df':[10,20,30],\n",
    "                     'vectorizador__binary':[False,True],\n",
    "                     'vectorizador__use_idf':[True,False],\n",
    "                     'clf__alpha': [0.001,0.01,0.1,0.5,1,1.5,2,2.5,3,3.5,4,4.5,5],\n",
    "                     'clf__normalize':[False,True],\n",
    "                     'clf__random_state':[42]}\n",
    "\n",
    "#Tuning de Hiperparametros\n",
    "clf_RR_tfidf_original = GridSearchCV(pipeline_tfidf_RR, hiperparametros_3,cv=5, n_jobs=-1,verbose=2)\n",
    "clf_RR_tfidf_original.fit(x_train_original, y_train)\n",
    "\n",
    "\n",
    "#Muestro los resultados\n",
    "\n",
    "print(\"Mejor Score: \", clf_RR_tfidf_original.best_score_)\n",
    "\n",
    "print(\"Mejores Parametros: \", clf_RR_tfidf_original.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generacion del SUBMIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leo el archivo .csv modelo que tenemos se utilizar para realizar el submit a Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creo una nueva columna con los valores que predice el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['target'] = clf_RR_tfidf_original.predict(x_test_original)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardo el archivo .csv para realizar el submit a Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"original_tfidf_hiper3_ridge_regression.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Datos Preprocesados**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 936 candidates, totalling 4680 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:    6.4s\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:   32.5s\n",
      "[Parallel(n_jobs=-1)]: Done 357 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 640 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 1005 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=-1)]: Done 1450 tasks      | elapsed:  4.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1977 tasks      | elapsed:  5.4min\n",
      "[Parallel(n_jobs=-1)]: Done 2584 tasks      | elapsed:  6.8min\n",
      "[Parallel(n_jobs=-1)]: Done 3273 tasks      | elapsed:  8.6min\n",
      "[Parallel(n_jobs=-1)]: Done 4042 tasks      | elapsed: 10.4min\n",
      "[Parallel(n_jobs=-1)]: Done 4680 out of 4680 | elapsed: 11.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor Score:  0.6997636764023059\n",
      "Mejores Parametros:  {'clf__alpha': 3, 'clf__normalize': True, 'clf__random_state': 42, 'vectorizador__binary': True, 'vectorizador__max_df': 30, 'vectorizador__ngram_range': (1, 2), 'vectorizador__strip_accents': 'unicode', 'vectorizador__use_idf': False}\n"
     ]
    }
   ],
   "source": [
    "#Dataset Preprocesado\n",
    "\n",
    "#Hiperparametros del vectorizador y del clasificador\n",
    "hiperparametros_4 = {'vectorizador__strip_accents':['unicode'],\n",
    "                     'vectorizador__ngram_range': [(1,1),(2,2),(1,2)],\n",
    "                     'vectorizador__max_df':[10,20,30],\n",
    "                     'vectorizador__binary':[False,True],\n",
    "                     'vectorizador__use_idf':[True,False],\n",
    "                     'clf__alpha': [0.001,0.01,0.1,0.5,1,1.5,2,2.5,3,3.5,4,4.5,5],\n",
    "                     'clf__normalize':[False,True],\n",
    "                     'clf__random_state':[42]}\n",
    "\n",
    "#Tuning de Hiperparametros\n",
    "clf_RR_tfidf_preprocesado = GridSearchCV(pipeline_tfidf_RR, hiperparametros_4,cv=5, n_jobs=-1,verbose=2)\n",
    "clf_RR_tfidf_preprocesado.fit(x_train_preprocesado, y_train)\n",
    "\n",
    "\n",
    "#Muestro los resultados\n",
    "\n",
    "print(\"Mejor Score: \", clf_RR_tfidf_preprocesado.best_score_)\n",
    "\n",
    "print(\"Mejores Parametros: \", clf_RR_tfidf_preprocesado.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generacion del SUBMIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leo el archivo .csv modelo que tenemos se utilizar para realizar el submit a Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creo una nueva columna con los valores que predice el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['target'] = clf_RR_tfidf_preprocesado.predict(x_test_preprocesado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardo el archivo .csv para realizar el submit a Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"preprocesado_tfidf_hiper3_ridge_regression.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.1 BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline\n",
    "\n",
    "pipeline_bow_LR = Pipeline([('vectorizador', CountVectorizer()),('clf', LogisticRegression())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Datos sin Preprocesar**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 504 candidates, totalling 2520 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:    7.9s\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:   26.1s\n",
      "[Parallel(n_jobs=-1)]: Done 357 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 640 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 1005 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1450 tasks      | elapsed:  4.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1977 tasks      | elapsed:  5.7min\n",
      "[Parallel(n_jobs=-1)]: Done 2520 out of 2520 | elapsed:  7.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor Score:  0.7276094931306857\n",
      "Mejores Parametros:  {'clf__C': 0.1, 'clf__penalty': 'l2', 'clf__random_state': 42, 'vectorizador__binary': True, 'vectorizador__min_df': 10, 'vectorizador__ngram_range': (1, 2), 'vectorizador__strip_accents': 'unicode'}\n"
     ]
    }
   ],
   "source": [
    "#Dataset sin preprocesamiento\n",
    "\n",
    "#Hiperparametros del vectorizador y del clasificador\n",
    "hiperparametros_5 = {'vectorizador__strip_accents':['unicode'],\n",
    "                     'vectorizador__ngram_range': [(1,1),(2,2),(1,2)],\n",
    "                     'vectorizador__min_df':[10,20,30],\n",
    "                     'vectorizador__binary':[False,True],\n",
    "                     'clf__C': [0.0001,0.001,0.01,0.1,0.5,1,1.5,2,2.5,3,3.5,4,4.5,5],\n",
    "                     'clf__penalty': ['l1', 'l2'],\n",
    "                     'clf__random_state':[42]}\n",
    "\n",
    "#Tuning de Hiperparametros\n",
    "clf_LR_bow_original = GridSearchCV(pipeline_bow_LR, hiperparametros_5,cv=5, n_jobs=-1,verbose=2)\n",
    "clf_LR_bow_original.fit(x_train_original, y_train)\n",
    "\n",
    "\n",
    "#Muestro los resultados\n",
    "\n",
    "print(\"Mejor Score: \", clf_LR_bow_original.best_score_)\n",
    "\n",
    "print(\"Mejores Parametros: \", clf_LR_bow_original.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generacion del SUBMIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leo el archivo .csv modelo que tenemos se utilizar para realizar el submit a Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creo una nueva columna con los valores que predice el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['target'] = clf_LR_bow_original.predict(x_test_original)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardo el archivo .csv para realizar el submit a Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"original_bow_hiper3_logistic_regression.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Datos Preprocesados**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 504 candidates, totalling 2520 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:    6.4s\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:   18.4s\n",
      "[Parallel(n_jobs=-1)]: Done 357 tasks      | elapsed:   38.1s\n",
      "[Parallel(n_jobs=-1)]: Done 640 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1005 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1450 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1977 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=-1)]: Done 2520 out of 2520 | elapsed:  4.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor Score:  0.693846716405347\n",
      "Mejores Parametros:  {'clf__C': 0.5, 'clf__penalty': 'l2', 'clf__random_state': 42, 'vectorizador__binary': False, 'vectorizador__min_df': 30, 'vectorizador__ngram_range': (1, 2), 'vectorizador__strip_accents': 'unicode'}\n"
     ]
    }
   ],
   "source": [
    "#Dataset Preprocesados\n",
    "\n",
    "#Hiperparametros del vectorizador y del clasificador\n",
    "hiperparametros_6 = {'vectorizador__strip_accents':['unicode'],\n",
    "                     'vectorizador__ngram_range': [(1,1),(2,2),(1,2)],\n",
    "                     'vectorizador__min_df':[10,20,30],\n",
    "                     'vectorizador__binary':[False,True],\n",
    "                     'clf__C': [0.0001,0.001,0.01,0.1,0.5,1,1.5,2,2.5,3,3.5,4,4.5,5],\n",
    "                     'clf__penalty': ['l1', 'l2'],\n",
    "                     'clf__random_state':[42]}\n",
    "\n",
    "#Tuning de Hiperparametros\n",
    "clf_LR_bow_preprocesado = GridSearchCV(pipeline_bow_LR, hiperparametros_6,cv=5, n_jobs=-1,verbose=2)\n",
    "clf_LR_bow_preprocesado.fit(x_train_preprocesado, y_train)\n",
    "\n",
    "\n",
    "#Muestro los resultados\n",
    "\n",
    "print(\"Mejor Score: \", clf_LR_bow_preprocesado.best_score_)\n",
    "\n",
    "print(\"Mejores Parametros: \", clf_LR_bow_preprocesado.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generacion del SUBMIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leo el archivo .csv modelo que tenemos se utilizar para realizar el submit a Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creo una nueva columna con los valores que predice el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['target'] = clf_LR_bow_preprocesado.predict(x_test_original)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardo el archivo .csv para realizar el submit a Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"preprocesado_bow_hiper3_logistic_regression.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.2 TFIDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline\n",
    "\n",
    "pipeline_tfidf_LR = Pipeline([('vectorizador', TfidfVectorizer()),('clf', LogisticRegression())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Datos sin Preprocesar**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1008 candidates, totalling 5040 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:    8.2s\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:   32.3s\n",
      "[Parallel(n_jobs=-1)]: Done 357 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 640 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1005 tasks      | elapsed:  3.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1450 tasks      | elapsed:  5.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1977 tasks      | elapsed:  8.2min\n",
      "[Parallel(n_jobs=-1)]: Done 2584 tasks      | elapsed: 11.5min\n",
      "[Parallel(n_jobs=-1)]: Done 3273 tasks      | elapsed: 15.5min\n",
      "[Parallel(n_jobs=-1)]: Done 4042 tasks      | elapsed: 20.2min\n",
      "[Parallel(n_jobs=-1)]: Done 4893 tasks      | elapsed: 25.7min\n",
      "[Parallel(n_jobs=-1)]: Done 5040 out of 5040 | elapsed: 27.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor Score:  0.7093148244860573\n",
      "Mejores Parametros:  {'clf__C': 5, 'clf__penalty': 'l2', 'clf__random_state': 42, 'vectorizador__binary': False, 'vectorizador__max_df': 30, 'vectorizador__ngram_range': (1, 2), 'vectorizador__strip_accents': 'unicode', 'vectorizador__use_idf': True}\n"
     ]
    }
   ],
   "source": [
    "#Dataset sin preprocesamiento\n",
    "\n",
    "#Hiperparametros del vectorizador y del clasificador\n",
    "hiperparametros_7 = {'vectorizador__strip_accents':['unicode'],\n",
    "                     'vectorizador__ngram_range': [(1,1),(2,2),(1,2)],\n",
    "                     'vectorizador__max_df':[10,20,30],\n",
    "                     'vectorizador__binary':[False,True],\n",
    "                     'vectorizador__use_idf':[True,False],\n",
    "                     'clf__C': [0.0001,0.001,0.01,0.1,0.5,1,1.5,2,2.5,3,3.5,4,4.5,5],\n",
    "                     'clf__penalty': ['l1', 'l2'],\n",
    "                     'clf__random_state':[42]}\n",
    "\n",
    "#Tuning de Hiperparametros\n",
    "clf_LR_tfidf_original = GridSearchCV(pipeline_tfidf_LR, hiperparametros_7,cv=5, n_jobs=-1,verbose=2)\n",
    "clf_LR_tfidf_original.fit(x_train_original, y_train)\n",
    "\n",
    "\n",
    "#Muestro los resultados\n",
    "\n",
    "print(\"Mejor Score: \", clf_LR_tfidf_original.best_score_)\n",
    "\n",
    "print(\"Mejores Parametros: \", clf_LR_tfidf_original.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generacion del SUBMIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leo el archivo .csv modelo que tenemos se utilizar para realizar el submit a Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creo una nueva columna con los valores que predice el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['target'] = clf_LR_tfidf_original.predict(x_test_original)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardo el archivo .csv para realizar el submit a Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"original_tfidf_hiper3_logistic_regression.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Datos Preprocesados**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1008 candidates, totalling 5040 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:    7.3s\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:   22.1s\n",
      "[Parallel(n_jobs=-1)]: Done 357 tasks      | elapsed:   56.4s\n",
      "[Parallel(n_jobs=-1)]: Done 640 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1005 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1450 tasks      | elapsed:  3.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1977 tasks      | elapsed:  5.1min\n",
      "[Parallel(n_jobs=-1)]: Done 2584 tasks      | elapsed:  7.5min\n",
      "[Parallel(n_jobs=-1)]: Done 3273 tasks      | elapsed: 10.5min\n",
      "[Parallel(n_jobs=-1)]: Done 4042 tasks      | elapsed: 13.4min\n",
      "[Parallel(n_jobs=-1)]: Done 4893 tasks      | elapsed: 16.8min\n",
      "[Parallel(n_jobs=-1)]: Done 5040 out of 5040 | elapsed: 17.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor Score:  0.6880613590552849\n",
      "Mejores Parametros:  {'clf__C': 2, 'clf__penalty': 'l2', 'clf__random_state': 42, 'vectorizador__binary': True, 'vectorizador__max_df': 30, 'vectorizador__ngram_range': (1, 2), 'vectorizador__strip_accents': 'unicode', 'vectorizador__use_idf': True}\n"
     ]
    }
   ],
   "source": [
    "#Dataset Preprocesado\n",
    "\n",
    "#Hiperparametros del vectorizador y del clasificador\n",
    "hiperparametros_8 = {'vectorizador__strip_accents':['unicode'],\n",
    "                     'vectorizador__ngram_range': [(1,1),(2,2),(1,2)],\n",
    "                     'vectorizador__max_df':[10,20,30],\n",
    "                     'vectorizador__binary':[False,True],\n",
    "                     'vectorizador__use_idf':[True,False],\n",
    "                     'clf__C': [0.0001,0.001,0.01,0.1,0.5,1,1.5,2,2.5,3,3.5,4,4.5,5],\n",
    "                     'clf__penalty': ['l1', 'l2'],\n",
    "                     'clf__random_state':[42]}\n",
    "\n",
    "#Tuning de Hiperparametros\n",
    "clf_LR_tfidf_preprocesado = GridSearchCV(pipeline_tfidf_LR, hiperparametros_8,cv=5, n_jobs=-1,verbose=2)\n",
    "clf_LR_tfidf_preprocesado.fit(x_train_preprocesado, y_train)\n",
    "\n",
    "\n",
    "#Muestro los resultados\n",
    "\n",
    "print(\"Mejor Score: \", clf_LR_tfidf_preprocesado.best_score_)\n",
    "\n",
    "print(\"Mejores Parametros: \", clf_LR_tfidf_preprocesado.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generacion del SUBMIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leo el archivo .csv modelo que tenemos se utilizar para realizar el submit a Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creo una nueva columna con los valores que predice el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['target'] = clf_LR_tfidf_preprocesado.predict(x_test_preprocesado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardo el archivo .csv para realizar el submit a Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"preprocesado_tfidf_hiper3_logistic_regression.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Naive Bayes (Multinomial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.1 BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline\n",
    "\n",
    "pipeline_bow_NB = Pipeline([('vectorizador', CountVectorizer()),('clf', MultinomialNB())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Datos sin Preprocesar**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 342 candidates, totalling 1710 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:    6.4s\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:   28.2s\n",
      "[Parallel(n_jobs=-1)]: Done 357 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 640 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 1005 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done 1450 tasks      | elapsed:  4.0min\n",
      "[Parallel(n_jobs=-1)]: Done 1710 out of 1710 | elapsed:  4.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor Score:  0.7184628376390811\n",
      "Mejores Parametros:  {'clf__alpha': 6, 'vectorizador__binary': False, 'vectorizador__min_df': 10, 'vectorizador__ngram_range': (1, 1), 'vectorizador__strip_accents': 'unicode'}\n"
     ]
    }
   ],
   "source": [
    "#Dataset sin preprocesamiento\n",
    "\n",
    "#Hiperparametros del vectorizador y del clasificador\n",
    "hiperparametros_9 = {'vectorizador__strip_accents':['unicode'],\n",
    "                     'vectorizador__ngram_range': [(1,1),(2,2),(1,2)],\n",
    "                     'vectorizador__min_df':[10,20,30],\n",
    "                     'vectorizador__binary':[False,True],\n",
    "                     'clf__alpha': [0.0001,0.001,0.01,0.05,0.1,1,1.5,2,2.5,3,3.5,4,4.5,5,6,7,8,9,10]}\n",
    "\n",
    "#Tuning de Hiperparametros\n",
    "clf_NB_bow_original = GridSearchCV(pipeline_bow_NB, hiperparametros_9,cv=5, n_jobs=-1,verbose=2)\n",
    "clf_NB_bow_original.fit(x_train_original, y_train)\n",
    "\n",
    "\n",
    "#Muestro los resultados\n",
    "\n",
    "print(\"Mejor Score: \", clf_NB_bow_original.best_score_)\n",
    "\n",
    "print(\"Mejores Parametros: \", clf_NB_bow_original.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generacion del SUBMIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leo el archivo .csv modelo que tenemos se utilizar para realizar el submit a Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creo una nueva columna con los valores que predice el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['target'] = clf_NB_bow_original.predict(x_test_original)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardo el archivo .csv para realizar el submit a Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"original_bow_hiper3_naive_bayes.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Datos Preprocesados**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 342 candidates, totalling 1710 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:    6.8s\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:   20.1s\n",
      "[Parallel(n_jobs=-1)]: Done 357 tasks      | elapsed:   43.2s\n",
      "[Parallel(n_jobs=-1)]: Done 640 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1005 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1450 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1710 out of 1710 | elapsed:  3.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor Score:  0.7029953631336998\n",
      "Mejores Parametros:  {'clf__alpha': 9, 'vectorizador__binary': False, 'vectorizador__min_df': 10, 'vectorizador__ngram_range': (1, 1), 'vectorizador__strip_accents': 'unicode'}\n"
     ]
    }
   ],
   "source": [
    "#Dataset Preprocesados\n",
    "\n",
    "#Hiperparametros del vectorizador y del clasificador\n",
    "hiperparametros_10 = {'vectorizador__strip_accents':['unicode'],\n",
    "                      'vectorizador__ngram_range': [(1,1),(2,2),(1,2)],\n",
    "                      'vectorizador__min_df':[10,20,30],\n",
    "                      'vectorizador__binary':[False,True],\n",
    "                      'clf__alpha': [0.0001,0.001,0.01,0.05,0.1,1,1.5,2,2.5,3,3.5,4,4.5,5,6,7,8,9,10]}\n",
    "\n",
    "#Tuning de Hiperparametros\n",
    "clf_NB_bow_preprocesado = GridSearchCV(pipeline_bow_NB, hiperparametros_10,cv=5, n_jobs=-1,verbose=2)\n",
    "clf_NB_bow_preprocesado.fit(x_train_preprocesado, y_train)\n",
    "\n",
    "\n",
    "#Muestro los resultados\n",
    "\n",
    "print(\"Mejor Score: \", clf_NB_bow_preprocesado.best_score_)\n",
    "\n",
    "print(\"Mejores Parametros: \", clf_NB_bow_preprocesado.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generacion del SUBMIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leo el archivo .csv modelo que tenemos se utilizar para realizar el submit a Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creo una nueva columna con los valores que predice el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['target'] = clf_NB_bow_preprocesado.predict(x_test_preprocesado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardo el archivo .csv para realizar el submit a Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"preprocesado_bow_hiper3_naive_bayes.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.2 TFIDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline\n",
    "\n",
    "pipeline_tfidf_NB = Pipeline([('vectorizador', TfidfVectorizer()),('clf', MultinomialNB())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Datos sin Preprocesar**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 684 candidates, totalling 3420 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:    7.3s\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:   35.1s\n",
      "[Parallel(n_jobs=-1)]: Done 357 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 640 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1005 tasks      | elapsed:  3.8min\n",
      "[Parallel(n_jobs=-1)]: Done 1450 tasks      | elapsed:  5.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1977 tasks      | elapsed:  7.4min\n",
      "[Parallel(n_jobs=-1)]: Done 2584 tasks      | elapsed:  9.7min\n",
      "[Parallel(n_jobs=-1)]: Done 3273 tasks      | elapsed: 12.4min\n",
      "[Parallel(n_jobs=-1)]: Done 3420 out of 3420 | elapsed: 12.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor Score:  0.7199424170536755\n",
      "Mejores Parametros:  {'clf__alpha': 1, 'vectorizador__binary': True, 'vectorizador__max_df': 30, 'vectorizador__ngram_range': (1, 2), 'vectorizador__strip_accents': 'unicode', 'vectorizador__use_idf': True}\n"
     ]
    }
   ],
   "source": [
    "#Dataset sin preprocesamiento\n",
    "\n",
    "#Hiperparametros del vectorizador y del clasificador\n",
    "hiperparametros_11 = {'vectorizador__strip_accents':['unicode'],\n",
    "                      'vectorizador__ngram_range': [(1,1),(2,2),(1,2)],\n",
    "                      'vectorizador__max_df':[10,20,30],\n",
    "                      'vectorizador__binary':[False,True],\n",
    "                      'vectorizador__use_idf':[True,False],\n",
    "                      'clf__alpha': [0.0001,0.001,0.01,0.05,0.1,1,1.5,2,2.5,3,3.5,4,4.5,5,6,7,8,9,10]}\n",
    "\n",
    "#Tuning de Hiperparametros\n",
    "clf_NB_tfidf_original = GridSearchCV(pipeline_tfidf_NB, hiperparametros_11,cv=5, n_jobs=-1,verbose=2)\n",
    "clf_NB_tfidf_original.fit(x_train_original, y_train)\n",
    "\n",
    "\n",
    "\n",
    "#Muestro los resultados\n",
    "\n",
    "print(\"Mejor Score: \", clf_NB_tfidf_original.best_score_)\n",
    "\n",
    "print(\"Mejores Parametros: \", clf_NB_tfidf_original.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generacion del SUBMIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leo el archivo .csv modelo que tenemos se utilizar para realizar el submit a Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creo una nueva columna con los valores que predice el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['target'] = clf_NB_tfidf_original.predict(x_test_original)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardo el archivo .csv para realizar el submit a Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"original_tfidf_hiper3_naive_bayes.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Datos Preprocesados**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 684 candidates, totalling 3420 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:    4.5s\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:   20.1s\n",
      "[Parallel(n_jobs=-1)]: Done 357 tasks      | elapsed:   46.1s\n",
      "[Parallel(n_jobs=-1)]: Done 640 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1005 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1450 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1977 tasks      | elapsed:  4.4min\n",
      "[Parallel(n_jobs=-1)]: Done 2584 tasks      | elapsed:  5.8min\n",
      "[Parallel(n_jobs=-1)]: Done 3273 tasks      | elapsed:  7.4min\n",
      "[Parallel(n_jobs=-1)]: Done 3420 out of 3420 | elapsed:  7.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor Score:  0.701512434820938\n",
      "Mejores Parametros:  {'clf__alpha': 1, 'vectorizador__binary': True, 'vectorizador__max_df': 30, 'vectorizador__ngram_range': (1, 2), 'vectorizador__strip_accents': 'unicode', 'vectorizador__use_idf': True}\n"
     ]
    }
   ],
   "source": [
    "#Dataset Preprocesado\n",
    "\n",
    "#Hiperparametros del vectorizador y del clasificador\n",
    "hiperparametros_12 = {'vectorizador__strip_accents':['unicode'],\n",
    "                      'vectorizador__ngram_range': [(1,1),(2,2),(1,2)],\n",
    "                      'vectorizador__max_df':[10,20,30],\n",
    "                      'vectorizador__binary':[False,True],\n",
    "                      'vectorizador__use_idf':[True,False],\n",
    "                      'clf__alpha': [0.0001,0.001,0.01,0.05,0.1,1,1.5,2,2.5,3,3.5,4,4.5,5,6,7,8,9,10]}\n",
    "\n",
    "#Tuning de Hiperparametros\n",
    "clf_NB_tfidf_preprocesado = GridSearchCV(pipeline_tfidf_NB, hiperparametros_12,cv=5, n_jobs=-1,verbose=2)\n",
    "clf_NB_tfidf_preprocesado.fit(x_train_preprocesado, y_train)\n",
    "\n",
    "\n",
    "\n",
    "#Muestro los resultados\n",
    "\n",
    "print(\"Mejor Score: \", clf_NB_tfidf_preprocesado.best_score_)\n",
    "\n",
    "print(\"Mejores Parametros: \", clf_NB_tfidf_preprocesado.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generacion del SUBMIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leo el archivo .csv modelo que tenemos se utilizar para realizar el submit a Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creo una nueva columna con los valores que predice el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['target'] = clf_NB_tfidf_preprocesado.predict(x_test_preprocesado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardo el archivo .csv para realizar el submit a Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"preprocesado_tfidf_hiper3_naive_bayes.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.1 BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline\n",
    "\n",
    "pipeline_bow_GB = Pipeline([('vectorizador', CountVectorizer()),('clf', GradientBoostingClassifier())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Datos sin Preprocesar**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 864 candidates, totalling 4320 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   19.3s\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 357 tasks      | elapsed:  3.5min\n",
      "[Parallel(n_jobs=-1)]: Done 640 tasks      | elapsed:  7.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1005 tasks      | elapsed: 13.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1450 tasks      | elapsed: 19.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1977 tasks      | elapsed: 27.6min\n",
      "[Parallel(n_jobs=-1)]: Done 2584 tasks      | elapsed: 36.0min\n",
      "[Parallel(n_jobs=-1)]: Done 3273 tasks      | elapsed: 46.5min\n",
      "[Parallel(n_jobs=-1)]: Done 4042 tasks      | elapsed: 56.0min\n",
      "[Parallel(n_jobs=-1)]: Done 4320 out of 4320 | elapsed: 61.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor Score:  0.7180561727886637\n",
      "Mejores Parametros:  {'clf__learning_rate': 0.075, 'clf__max_depth': 6, 'clf__n_estimators': 300, 'clf__random_state': 42, 'vectorizador__binary': True, 'vectorizador__min_df': 30, 'vectorizador__ngram_range': (1, 1), 'vectorizador__strip_accents': 'unicode'}\n"
     ]
    }
   ],
   "source": [
    "#Dataset sin preprocesamiento\n",
    "\n",
    "#Hiperparametros del vectorizador y del clasificador\n",
    "hiperparametros_13 = {'vectorizador__strip_accents':['unicode'],\n",
    "                      'vectorizador__ngram_range': [(1,1),(2,2),(1,2)],\n",
    "                      'vectorizador__min_df':[10,20,30],\n",
    "                      'vectorizador__binary':[False,True],\n",
    "                      'clf__learning_rate': [0.075,0.1,0.125,0.15],\n",
    "                      'clf__n_estimators': [100,200,300],\n",
    "                      'clf__max_depth': [3,4,5,6],\n",
    "                      'clf__random_state':[42]}\n",
    "\n",
    "#Tuning de Hiperparametros\n",
    "clf_GB_bow_original = GridSearchCV(pipeline_bow_GB, hiperparametros_13,cv=5, n_jobs=-1,verbose=2)\n",
    "clf_GB_bow_original.fit(x_train_original, y_train)\n",
    "\n",
    "\n",
    "#Muestro los resultados\n",
    "\n",
    "print(\"Mejor Score: \", clf_GB_bow_original.best_score_)\n",
    "\n",
    "print(\"Mejores Parametros: \", clf_GB_bow_original.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generacion del SUBMIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leo el archivo .csv modelo que tenemos se utilizar para realizar el submit a Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creo una nueva columna con los valores que predice el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['target'] = clf_GB_bow_original.predict(x_test_original)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardo el archivo .csv para realizar el submit a Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"original_bow_hiper3_gradient_boosting.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Datos Preprocesados**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 864 candidates, totalling 4320 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   12.4s\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:   54.7s\n",
      "[Parallel(n_jobs=-1)]: Done 357 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 640 tasks      | elapsed:  4.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1005 tasks      | elapsed:  8.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1450 tasks      | elapsed: 12.0min\n",
      "[Parallel(n_jobs=-1)]: Done 1977 tasks      | elapsed: 16.6min\n",
      "[Parallel(n_jobs=-1)]: Done 2584 tasks      | elapsed: 21.6min\n",
      "[Parallel(n_jobs=-1)]: Done 3273 tasks      | elapsed: 28.2min\n",
      "[Parallel(n_jobs=-1)]: Done 4042 tasks      | elapsed: 34.6min\n",
      "[Parallel(n_jobs=-1)]: Done 4320 out of 4320 | elapsed: 37.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor Score:  0.6903503762079792\n",
      "Mejores Parametros:  {'clf__learning_rate': 0.15, 'clf__max_depth': 6, 'clf__n_estimators': 300, 'clf__random_state': 42, 'vectorizador__binary': True, 'vectorizador__min_df': 30, 'vectorizador__ngram_range': (1, 2), 'vectorizador__strip_accents': 'unicode'}\n"
     ]
    }
   ],
   "source": [
    "#Dataset Preprocesado\n",
    "\n",
    "#Hiperparametros del vectorizador y del clasificador\n",
    "hiperparametros_14 = {'vectorizador__strip_accents':['unicode'],\n",
    "                      'vectorizador__ngram_range': [(1,1),(2,2),(1,2)],\n",
    "                      'vectorizador__min_df':[10,20,30],\n",
    "                      'vectorizador__binary':[False,True],\n",
    "                      'clf__learning_rate': [0.075,0.1,0.125,0.15],\n",
    "                      'clf__n_estimators': [100,200,300],\n",
    "                      'clf__max_depth': [3,4,5,6],\n",
    "                      'clf__random_state':[42]}\n",
    "\n",
    "#Tuning de Hiperparametros\n",
    "clf_GB_bow_preprocesado = GridSearchCV(pipeline_bow_GB, hiperparametros_14,cv=5, n_jobs=-1,verbose=2)\n",
    "clf_GB_bow_preprocesado.fit(x_train_preprocesado, y_train)\n",
    "\n",
    "\n",
    "#Muestro los resultados\n",
    "\n",
    "print(\"Mejor Score: \", clf_GB_bow_preprocesado.best_score_)\n",
    "\n",
    "print(\"Mejores Parametros: \", clf_GB_bow_preprocesado.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generacion del SUBMIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leo el archivo .csv modelo que tenemos se utilizar para realizar el submit a Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creo una nueva columna con los valores que predice el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['target'] = clf_GB_bow_preprocesado.predict(x_test_preprocesado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardo el archivo .csv para realizar el submit a Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"preprocesado_bow_hiper3_gradient_boosting.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.2 TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline\n",
    "\n",
    "pipeline_tfidf_GB = Pipeline([('vectorizador', TfidfVectorizer()),('clf', GradientBoostingClassifier())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Datos sin Preprocesar**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1296 candidates, totalling 6480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed: 10.8min\n",
      "[Parallel(n_jobs=-1)]: Done 357 tasks      | elapsed: 96.9min\n",
      "[Parallel(n_jobs=-1)]: Done 640 tasks      | elapsed: 158.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1005 tasks      | elapsed: 209.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1450 tasks      | elapsed: 273.8min\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-49-8894ddc06066>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m#Tuning de Hiperparametros\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mclf_GB_tfidf_original\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpipeline_tfidf_GB\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhiperparametros_15\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mclf_GB_tfidf_original\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train_original\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     71\u001b[0m                           FutureWarning)\n\u001b[0;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    734\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    735\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 736\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    737\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    738\u001b[0m         \u001b[1;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1186\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1187\u001b[0m         \u001b[1;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1188\u001b[1;33m         \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1189\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params)\u001b[0m\n\u001b[0;32m    706\u001b[0m                               n_splits, n_candidates, n_candidates * n_splits))\n\u001b[0;32m    707\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 708\u001b[1;33m                 out = parallel(delayed(_fit_and_score)(clone(base_estimator),\n\u001b[0m\u001b[0;32m    709\u001b[0m                                                        \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    710\u001b[0m                                                        \u001b[0mtrain\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1042\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1043\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1044\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    919\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    920\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'supports_timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 921\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    922\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    923\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[0;32m    541\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 542\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    543\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    432\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    433\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 434\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    435\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    436\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    300\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 302\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    303\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Dataset sin preprocesamiento\n",
    "\n",
    "#Hiperparametros del vectorizador y del clasificador\n",
    "hiperparametros_15 = {'vectorizador__strip_accents':['unicode'],\n",
    "                      'vectorizador__ngram_range': [(1,1),(2,2),(1,2)],\n",
    "                      'vectorizador__max_df':[10,20,30],\n",
    "                      'vectorizador__binary':[False,True],\n",
    "                      'vectorizador__use_idf':[True,False],\n",
    "                      'clf__learning_rate': [0.1,0.125,0.15],\n",
    "                      'clf__n_estimators': [100,200,300],\n",
    "                      'clf__max_depth': [3,4,5,6],\n",
    "                      'clf__random_state':[42]}\n",
    "\n",
    "#Tuning de Hiperparametros\n",
    "clf_GB_tfidf_original = GridSearchCV(pipeline_tfidf_GB, hiperparametros_15,cv=5, n_jobs=-1,verbose=2)\n",
    "clf_GB_tfidf_original.fit(x_train_original, y_train)\n",
    "\n",
    "\n",
    "\n",
    "#Muestro los resultados\n",
    "\n",
    "print(\"Mejor Score: \", clf_GB_tfidf_original.best_score_)\n",
    "\n",
    "print(\"Mejores Parametros: \", clf_GB_tfidf_original.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generacion del SUBMIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leo el archivo .csv modelo que tenemos se utilizar para realizar el submit a Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creo una nueva columna con los valores que predice el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['target'] = clf_GB_tfidf_original.predict(x_test_original)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardo el archivo .csv para realizar el submit a Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"original_tfidf_hiper3_gradient_boosting.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Datos Preprocesados**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 684 candidates, totalling 3420 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:    4.5s\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:   20.1s\n",
      "[Parallel(n_jobs=-1)]: Done 357 tasks      | elapsed:   46.1s\n",
      "[Parallel(n_jobs=-1)]: Done 640 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1005 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1450 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1977 tasks      | elapsed:  4.4min\n",
      "[Parallel(n_jobs=-1)]: Done 2584 tasks      | elapsed:  5.8min\n",
      "[Parallel(n_jobs=-1)]: Done 3273 tasks      | elapsed:  7.4min\n",
      "[Parallel(n_jobs=-1)]: Done 3420 out of 3420 | elapsed:  7.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor Score:  0.701512434820938\n",
      "Mejores Parametros:  {'clf__alpha': 1, 'vectorizador__binary': True, 'vectorizador__max_df': 30, 'vectorizador__ngram_range': (1, 2), 'vectorizador__strip_accents': 'unicode', 'vectorizador__use_idf': True}\n"
     ]
    }
   ],
   "source": [
    "#Dataset Preprocesado\n",
    "\n",
    "#Hiperparametros del vectorizador y del clasificador\n",
    "hiperparametros_16 = {'vectorizador__strip_accents':['unicode'],\n",
    "                      'vectorizador__ngram_range': [(1,1),(2,2),(1,2)],\n",
    "                      'vectorizador__max_df':[10,20,30],\n",
    "                      'vectorizador__binary':[False,True],\n",
    "                      'vectorizador__use_idf':[True,False],\n",
    "                      'clf__learning_rate': [0.1,0.125,0.15],\n",
    "                      'clf__n_estimators': [100,200,300],\n",
    "                      'clf__max_depth': [3,4,5,6],\n",
    "                      'clf__random_state':[42]}\n",
    "\n",
    "#Tuning de Hiperparametros\n",
    "clf_GB_tfidf_preprocesado = GridSearchCV(pipeline_tfidf_GB, hiperparametros_16,cv=5, n_jobs=-1,verbose=2)\n",
    "clf_GB_tfidf_preprocesado.fit(x_train_preprocesado, y_train)\n",
    "\n",
    "\n",
    "\n",
    "#Muestro los resultados\n",
    "\n",
    "print(\"Mejor Score: \", clf_GB_tfidf_preprocesado.best_score_)\n",
    "\n",
    "print(\"Mejores Parametros: \", clf_GB_tfidf_preprocesado.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generacion del SUBMIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leo el archivo .csv modelo que tenemos se utilizar para realizar el submit a Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creo una nueva columna con los valores que predice el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['target'] = clf_GB_tfidf_preprocesado.predict(x_test_preprocesado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardo el archivo .csv para realizar el submit a Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"preprocesado_tfidf_hiper3_gradient_boosting.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  5.5 Linear Support Vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5.1 BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline\n",
    "\n",
    "pipeline_bow_SV = Pipeline([('vectorizador', CountVectorizer()),('clf', LinearSVC())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Datos sin Preprocesar**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:    7.4s\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:   32.8s\n",
      "[Parallel(n_jobs=-1)]: Done 357 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 540 out of 540 | elapsed:  2.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor Score:  0.7273414907665445\n",
      "Mejores Parametros:  {'clf__C': 0.01, 'clf__random_state': 42, 'vectorizador__binary': True, 'vectorizador__min_df': 10, 'vectorizador__ngram_range': (1, 1), 'vectorizador__strip_accents': 'unicode'}\n"
     ]
    }
   ],
   "source": [
    "#Dataset sin preprocesamiento\n",
    "\n",
    "#Hiperparametros del vectorizador y del clasificador\n",
    "hiperparametros_25 = {'vectorizador__strip_accents':['unicode'],\n",
    "                      'vectorizador__ngram_range': [(1,1),(2,2),(1,2)],\n",
    "                      'vectorizador__min_df':[10,20,30],\n",
    "                      'vectorizador__binary':[False,True],\n",
    "                      'clf__C': [0.001,0.01,0.25,0.5,0.75,1],\n",
    "                      'clf__random_state':[42]}\n",
    "\n",
    "#Tuning de Hiperparametros\n",
    "clf_SV_bow_original = GridSearchCV(pipeline_bow_SV, hiperparametros_25,cv=5, n_jobs=-1,verbose=2)\n",
    "clf_SV_bow_original.fit(x_train_original, y_train)\n",
    "\n",
    "\n",
    "#Muestro los resultados\n",
    "\n",
    "print(\"Mejor Score: \", clf_SV_bow_original.best_score_)\n",
    "\n",
    "print(\"Mejores Parametros: \", clf_SV_bow_original.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generacion del SUBMIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leo el archivo .csv modelo que tenemos se utilizar para realizar el submit a Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creo una nueva columna con los valores que predice el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['target'] = clf_SV_bow_original.predict(x_test_original)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardo el archivo .csv para realizar el submit a Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"original_bow_hiper3_linear_support_vector.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Datos Preprocesados**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:    4.7s\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:   19.3s\n",
      "[Parallel(n_jobs=-1)]: Done 357 tasks      | elapsed:   49.8s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor Score:  0.689273479170306\n",
      "Mejores Parametros:  {'clf__C': 0.01, 'clf__random_state': 42, 'vectorizador__binary': True, 'vectorizador__min_df': 10, 'vectorizador__ngram_range': (1, 1)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 540 out of 540 | elapsed:  1.4min finished\n"
     ]
    }
   ],
   "source": [
    "#Dataset Preprocesado\n",
    "\n",
    "#Hiperparametros del vectorizador y del clasificador\n",
    "hiperparametros_26 = {'vectorizador__ngram_range': [(1,1),(2,2),(1,2)],\n",
    "                      'vectorizador__min_df':[10,20,30],\n",
    "                      'vectorizador__binary':[False,True],\n",
    "                      'clf__C': [0.001,0.01,0.25,0.5,0.75,1],\n",
    "                      'clf__random_state':[42]}\n",
    "\n",
    "#Tuning de Hiperparametros\n",
    "clf_SV_bow_preprocesado = GridSearchCV(pipeline_bow_SV, hiperparametros_26,cv=5, n_jobs=-1,verbose=2)\n",
    "clf_SV_bow_preprocesado.fit(x_train_preprocesado, y_train)\n",
    "\n",
    "\n",
    "#Muestro los resultados\n",
    "\n",
    "print(\"Mejor Score: \", clf_SV_bow_preprocesado.best_score_)\n",
    "\n",
    "print(\"Mejores Parametros: \", clf_SV_bow_preprocesado.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generacion del SUBMIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leo el archivo .csv modelo que tenemos se utilizar para realizar el submit a Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creo una nueva columna con los valores que predice el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['target'] = clf_SV_bow_preprocesado.predict(x_test_preprocesado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardo el archivo .csv para realizar el submit a Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"preprocesado_bow_hiper3_linear_support_vector.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5.2 TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline\n",
    "\n",
    "pipeline_tfidf_SV = Pipeline([('vectorizador', TfidfVectorizer()),('clf', LinearSVC())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Datos sin Preprocesar**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 216 candidates, totalling 1080 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   11.7s\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:   49.1s\n",
      "[Parallel(n_jobs=-1)]: Done 357 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 640 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1005 tasks      | elapsed:  5.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1080 out of 1080 | elapsed:  6.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor Score:  0.7066244826178609\n",
      "Mejores Parametros:  {'clf__C': 0.25, 'clf__random_state': 42, 'vectorizador__binary': True, 'vectorizador__max_df': 30, 'vectorizador__ngram_range': (1, 2), 'vectorizador__strip_accents': 'unicode', 'vectorizador__use_idf': True}\n"
     ]
    }
   ],
   "source": [
    "#Dataset sin preprocesamiento\n",
    "\n",
    "#Hiperparametros del vectorizador y del clasificador\n",
    "hiperparametros_27 = {'vectorizador__strip_accents':['unicode'],\n",
    "                      'vectorizador__ngram_range': [(1,1),(2,2),(1,2)],\n",
    "                      'vectorizador__max_df':[10,20,30],\n",
    "                      'vectorizador__binary':[False,True],\n",
    "                      'vectorizador__use_idf':[True,False],\n",
    "                      'clf__C': [0.001,0.01,0.25,0.5,0.75,1],\n",
    "                      'clf__random_state':[42]}\n",
    "\n",
    "#Tuning de Hiperparametros\n",
    "clf_SV_tfidf_original = GridSearchCV(pipeline_tfidf_SV,hiperparametros_27,cv=5, n_jobs=-1,verbose=2)\n",
    "clf_SV_tfidf_original.fit(x_train_original, y_train)\n",
    "\n",
    "\n",
    "\n",
    "#Muestro los resultados\n",
    "\n",
    "print(\"Mejor Score: \", clf_SV_tfidf_original.best_score_)\n",
    "\n",
    "print(\"Mejores Parametros: \", clf_SV_tfidf_original.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generacion del SUBMIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leo el archivo .csv modelo que tenemos se utilizar para realizar el submit a Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creo una nueva columna con los valores que predice el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['target'] = clf_SV_tfidf_original.predict(x_test_original)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardo el archivo .csv para realizar el submit a Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"original_tfidf_hiper3_linear_support_vector.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Datos Preprocesados**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 216 candidates, totalling 1080 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:    5.9s\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:   29.6s\n",
      "[Parallel(n_jobs=-1)]: Done 357 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 640 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1005 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1080 out of 1080 | elapsed:  3.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor Score:  0.6891368079207777\n",
      "Mejores Parametros:  {'clf__C': 0.25, 'clf__random_state': 42, 'vectorizador__binary': True, 'vectorizador__max_df': 30, 'vectorizador__ngram_range': (1, 2), 'vectorizador__strip_accents': 'unicode', 'vectorizador__use_idf': True}\n"
     ]
    }
   ],
   "source": [
    "#Dataset Preprocesado\n",
    "\n",
    "#Hiperparametros del vectorizador y del clasificador\n",
    "hiperparametros_28 = {'vectorizador__strip_accents':['unicode'],\n",
    "                      'vectorizador__ngram_range': [(1,1),(2,2),(1,2)],\n",
    "                      'vectorizador__max_df':[10,20,30],\n",
    "                      'vectorizador__binary':[False,True],\n",
    "                      'vectorizador__use_idf':[True,False],\n",
    "                      'clf__C': [0.001,0.01,0.25,0.5,0.75,1],\n",
    "                      'clf__random_state':[42]}\n",
    "\n",
    "#Tuning de Hiperparametros\n",
    "clf_SV_tfidf_preprocesado = GridSearchCV(pipeline_tfidf_SV,hiperparametros_28,cv=5, n_jobs=-1,verbose=2)\n",
    "clf_SV_tfidf_preprocesado.fit(x_train_preprocesado, y_train)\n",
    "\n",
    "\n",
    "\n",
    "#Muestro los resultados\n",
    "\n",
    "print(\"Mejor Score: \", clf_SV_tfidf_preprocesado.best_score_)\n",
    "\n",
    "print(\"Mejores Parametros: \", clf_SV_tfidf_preprocesado.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generacion del SUBMIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leo el archivo .csv modelo que tenemos se utilizar para realizar el submit a Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creo una nueva columna con los valores que predice el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['target'] = clf_SV_tfidf_preprocesado.predict(x_test_preprocesado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardo el archivo .csv para realizar el submit a Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"preprocesado_tfidf_hiper3_linear_support_vector.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  5.6 C-Support Vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6.1 BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline\n",
    "\n",
    "pipeline_bow_SVC = Pipeline([('vectorizador', CountVectorizer()),('clf', SVC())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Datos sin Preprocesar**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 432 candidates, totalling 2160 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:  6.3min\n",
      "[Parallel(n_jobs=-1)]: Done 357 tasks      | elapsed: 11.1min\n",
      "[Parallel(n_jobs=-1)]: Done 640 tasks      | elapsed: 20.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1005 tasks      | elapsed: 29.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1450 tasks      | elapsed: 38.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1977 tasks      | elapsed: 54.3min\n",
      "[Parallel(n_jobs=-1)]: Done 2160 out of 2160 | elapsed: 57.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor Score:  0.7284142243091993\n",
      "Mejores Parametros:  {'clf__C': 0.25, 'clf__kernel': 'sigmoid', 'clf__random_state': 42, 'vectorizador__binary': True, 'vectorizador__min_df': 20, 'vectorizador__ngram_range': (1, 2), 'vectorizador__strip_accents': 'unicode'}\n"
     ]
    }
   ],
   "source": [
    "#Dataset sin preprocesamiento\n",
    "\n",
    "#Hiperparametros del vectorizador y del clasificador\n",
    "hiperparametros_29 = {'vectorizador__strip_accents':['unicode'],\n",
    "                      'vectorizador__ngram_range': [(1,1),(2,2),(1,2)],\n",
    "                      'vectorizador__min_df':[10,20,30],\n",
    "                      'vectorizador__binary':[False,True],\n",
    "                      'clf__C': [0.001,0.01,0.25,0.5,0.75,1],\n",
    "                      'clf__kernel':['poly', 'rbf', 'sigmoid', 'precomputed'],\n",
    "                      'clf__random_state':[42]}\n",
    "\n",
    "#Tuning de Hiperparametros\n",
    "clf_SVC_bow_original = GridSearchCV(pipeline_bow_SVC, hiperparametros_29,cv=5, n_jobs=-1,verbose=2)\n",
    "clf_SVC_bow_original.fit(x_train_original, y_train)\n",
    "\n",
    "\n",
    "#Muestro los resultados\n",
    "\n",
    "print(\"Mejor Score: \", clf_SVC_bow_original.best_score_)\n",
    "\n",
    "print(\"Mejores Parametros: \", clf_SVC_bow_original.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generacion del SUBMIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leo el archivo .csv modelo que tenemos se utilizar para realizar el submit a Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creo una nueva columna con los valores que predice el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['target'] = clf_SVC_bow_original.predict(x_test_original)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardo el archivo .csv para realizar el submit a Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"original_bow_hiper3_support_vector.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Datos Preprocesados**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 432 candidates, totalling 2160 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   46.4s\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=-1)]: Done 357 tasks      | elapsed:  5.5min\n",
      "[Parallel(n_jobs=-1)]: Done 640 tasks      | elapsed: 10.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1005 tasks      | elapsed: 15.8min\n",
      "[Parallel(n_jobs=-1)]: Done 1450 tasks      | elapsed: 22.0min\n",
      "[Parallel(n_jobs=-1)]: Done 1977 tasks      | elapsed: 29.3min\n",
      "[Parallel(n_jobs=-1)]: Done 2160 out of 2160 | elapsed: 30.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor Score:  0.6900815592469867\n",
      "Mejores Parametros:  {'clf__C': 0.75, 'clf__kernel': 'rbf', 'clf__random_state': 42, 'vectorizador__binary': False, 'vectorizador__min_df': 30, 'vectorizador__ngram_range': (1, 2), 'vectorizador__strip_accents': 'unicode'}\n"
     ]
    }
   ],
   "source": [
    "#Dataset Preprocesado\n",
    "\n",
    "#Hiperparametros del vectorizador y del clasificador\n",
    "hiperparametros_30 = {'vectorizador__strip_accents':['unicode'],\n",
    "                      'vectorizador__ngram_range': [(1,1),(2,2),(1,2)],\n",
    "                      'vectorizador__min_df':[10,20,30],\n",
    "                      'vectorizador__binary':[False,True],\n",
    "                      'clf__C': [0.001,0.01,0.25,0.5,0.75,1],\n",
    "                      'clf__kernel':['poly', 'rbf', 'sigmoid', 'precomputed'],\n",
    "                      'clf__random_state':[42]}\n",
    "\n",
    "#Tuning de Hiperparametros\n",
    "clf_SVC_bow_preprocesado = GridSearchCV(pipeline_bow_SVC, hiperparametros_30,cv=5, n_jobs=-1,verbose=2)\n",
    "clf_SVC_bow_preprocesado.fit(x_train_preprocesado, y_train)\n",
    "\n",
    "\n",
    "#Muestro los resultados\n",
    "\n",
    "print(\"Mejor Score: \", clf_SVC_bow_preprocesado.best_score_)\n",
    "\n",
    "print(\"Mejores Parametros: \", clf_SVC_bow_preprocesado.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generacion del SUBMIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leo el archivo .csv modelo que tenemos se utilizar para realizar el submit a Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creo una nueva columna con los valores que predice el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['target'] = clf_SV_bow_preprocesado.predict(x_test_preprocesado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardo el archivo .csv para realizar el submit a Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"preprocesado_bow_hiper3_support_vector.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6.2 TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline\n",
    "\n",
    "pipeline_tfidf_SVC = Pipeline([('vectorizador', TfidfVectorizer()),('clf', SVC())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Datos sin Preprocesar**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 144 candidates, totalling 720 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:  6.8min\n",
      "[Parallel(n_jobs=-1)]: Done 357 tasks      | elapsed: 17.0min\n",
      "[Parallel(n_jobs=-1)]: Done 640 tasks      | elapsed: 26.0min\n",
      "[Parallel(n_jobs=-1)]: Done 720 out of 720 | elapsed: 26.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor Score:  0.7046053685552944\n",
      "Mejores Parametros:  {'clf__C': 1, 'clf__kernel': 'sigmoid', 'clf__random_state': 42, 'vectorizador__binary': True, 'vectorizador__max_df': 30, 'vectorizador__ngram_range': (1, 2), 'vectorizador__strip_accents': 'unicode', 'vectorizador__use_idf': True}\n"
     ]
    }
   ],
   "source": [
    "#Dataset sin preprocesamiento\n",
    "\n",
    "#Hiperparametros del vectorizador y del clasificador\n",
    "hiperparametros_31 = {'vectorizador__strip_accents':['unicode'],\n",
    "                      'vectorizador__ngram_range': [(1,1),(2,2),(1,2)],\n",
    "                      'vectorizador__max_df':[10,20,30],\n",
    "                      'vectorizador__binary':[False,True],\n",
    "                      'vectorizador__use_idf':[True,False],\n",
    "                      'clf__C': [1],\n",
    "                      'clf__kernel':['poly', 'rbf', 'sigmoid', 'precomputed'],\n",
    "                      'clf__random_state':[42]}\n",
    "\n",
    "#Tuning de Hiperparametros\n",
    "clf_SVC_tfidf_original = GridSearchCV(pipeline_tfidf_SVC,hiperparametros_31,cv=5, n_jobs=-1,verbose=2)\n",
    "clf_SVC_tfidf_original.fit(x_train_original, y_train)\n",
    "\n",
    "\n",
    "\n",
    "#Muestro los resultados\n",
    "\n",
    "print(\"Mejor Score: \", clf_SVC_tfidf_original.best_score_)\n",
    "\n",
    "print(\"Mejores Parametros: \", clf_SVC_tfidf_original.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generacion del SUBMIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leo el archivo .csv modelo que tenemos se utilizar para realizar el submit a Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creo una nueva columna con los valores que predice el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['target'] = clf_SVC_tfidf_original.predict(x_test_original)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardo el archivo .csv para realizar el submit a Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"original_tfidf_hiper3_support_vector.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Datos Preprocesados**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 144 candidates, totalling 720 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:  5.9min\n",
      "[Parallel(n_jobs=-1)]: Done 357 tasks      | elapsed: 13.7min\n",
      "[Parallel(n_jobs=-1)]: Done 640 tasks      | elapsed: 19.0min\n",
      "[Parallel(n_jobs=-1)]: Done 720 out of 720 | elapsed: 19.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor Score:  0.6833514505707156\n",
      "Mejores Parametros:  {'clf__C': 1, 'clf__kernel': 'rbf', 'clf__random_state': 42, 'vectorizador__binary': True, 'vectorizador__max_df': 30, 'vectorizador__ngram_range': (1, 1), 'vectorizador__strip_accents': 'unicode', 'vectorizador__use_idf': True}\n"
     ]
    }
   ],
   "source": [
    "#Dataset Preprocesado\n",
    "\n",
    "#Hiperparametros del vectorizador y del clasificador\n",
    "hiperparametros_32 = {'vectorizador__strip_accents':['unicode'],\n",
    "                      'vectorizador__ngram_range': [(1,1),(2,2),(1,2)],\n",
    "                      'vectorizador__max_df':[10,20,30],\n",
    "                      'vectorizador__binary':[False,True],\n",
    "                      'vectorizador__use_idf':[True,False],\n",
    "                      'clf__C': [1],\n",
    "                      'clf__kernel':['poly', 'rbf', 'sigmoid', 'precomputed'],\n",
    "                      'clf__random_state':[42]}\n",
    "\n",
    "#Tuning de Hiperparametros\n",
    "clf_SVC_tfidf_preprocesado = GridSearchCV(pipeline_tfidf_SVC,hiperparametros_32,cv=5, n_jobs=-1,verbose=2)\n",
    "clf_SVC_tfidf_preprocesado.fit(x_train_preprocesado, y_train)\n",
    "\n",
    "\n",
    "#Muestro los resultados\n",
    "\n",
    "print(\"Mejor Score: \", clf_SVC_tfidf_preprocesado.best_score_)\n",
    "\n",
    "print(\"Mejores Parametros: \", clf_SVC_tfidf_preprocesado.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generacion del SUBMIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leo el archivo .csv modelo que tenemos se utilizar para realizar el submit a Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creo una nueva columna con los valores que predice el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['target'] = clf_SVC_tfidf_preprocesado.predict(x_test_preprocesado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardo el archivo .csv para realizar el submit a Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"preprocesado_tfidf_hiper3_support_vector.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
