{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#librerías, no es necesario volverlas a importar\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.optimizers.schedules import ExponentialDecay\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers, Sequential\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/celeste/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/celeste/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/celeste/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "data_loader = os.path.join('..','0_Data','0_DataLoader.ipynb')\n",
    "#Vectorizacion\n",
    "%run $data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    acc = history.history['accuracy']\n",
    "    loss = history.history['loss']\n",
    "    x = range(1, len(acc) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.plot(x, acc, 'r', label='Training acc')\n",
    "    plt.plot(x, loss, 'b', label='Training loss')\n",
    "    plt.title('Training accuracy vs loss')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model (train_x, train_y, val_x, val_y, test_x, test_y,\n",
    "               embed_size, max_features, max_len, \n",
    "               batch_size, activationl1, nneuronsl1, epochs ):\n",
    "    cores = multiprocessing.cpu_count() # Count the number of cores in a computer\n",
    "    \n",
    "    ## Tokenize the sentences\n",
    "    tokenizer = Tokenizer(num_words=max_features)\n",
    "    tokenizer.fit_on_texts(list(train_x) + list(val_x))\n",
    "\n",
    "    train_X = tokenizer.texts_to_sequences(train_x)\n",
    "    train_X = pad_sequences(train_X, maxlen=max_len)\n",
    "\n",
    "    val_X = tokenizer.texts_to_sequences(val_x)\n",
    "    val_X = pad_sequences(val_X, maxlen=max_len)\n",
    "    \n",
    "    test_X = tokenizer.texts_to_sequences(test_x)\n",
    "    test_X = pad_sequences(test_X, maxlen=max_len)\n",
    "    \n",
    "    \n",
    "    ## indice del vocabulario\n",
    "    word_index = tokenizer.word_index\n",
    "\n",
    "    ## para proecesarlo necesito una lista de palabras por  tweet\n",
    "    tweets_list_train = [text_to_word_sequence(sentence) for sentence in train_x]\n",
    "    tweets_list_val = [text_to_word_sequence(sentence) for sentence in val_x]\n",
    "    \n",
    "    tweets_all = tweets_list_train + tweets_list_val \n",
    "\n",
    "    ## creo el modelo de word2vec\n",
    "    w2v_model = Word2Vec(min_count=20,\n",
    "                         window=2,\n",
    "                         size=embed_size,\n",
    "                         sample=6e-5, \n",
    "                         alpha=0.03, \n",
    "                         min_alpha=0.0007, \n",
    "                         negative=20,\n",
    "                         workers=cores-1)\n",
    "\n",
    "    ## se genera el vocabulario\n",
    "\n",
    "    w2v_model.build_vocab(tweets_all, progress_per=10000)\n",
    "\n",
    "    # se entrena el modelo word2vec\n",
    "\n",
    "    w2v_model.train(tweets_all, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)\n",
    "\n",
    "    word_vectors = w2v_model.wv\n",
    "\n",
    "    #print(\"Number of word vectors: {}\".format(len(word_vectors.vocab)))\n",
    "\n",
    "    MAX_NB_WORDS = min(max_features, len(word_index)+1)\n",
    "\n",
    "    wv_matrix = (np.random.rand(MAX_NB_WORDS, embed_size) - 0.5) / 5.0\n",
    "    for word, i in word_index.items():\n",
    "        if i >= MAX_NB_WORDS:\n",
    "            continue\n",
    "        try:\n",
    "            embedding_vector = word_vectors[word]\n",
    "            wv_matrix[i] = embedding_vector\n",
    "        except:\n",
    "            pass    \n",
    "\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(layers.Embedding(input_dim=MAX_NB_WORDS, \n",
    "                               output_dim=embed_size, \n",
    "                               input_length=max_len,\n",
    "                              weights=[wv_matrix]))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(nneuronsl1, activation=activationl1))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    lr_schedule = ExponentialDecay(\n",
    "        0.001,\n",
    "        decay_steps=100000,\n",
    "        decay_rate=0.96,\n",
    "        staircase=True)\n",
    "    opt = Adam(learning_rate=lr_schedule)\n",
    "    #opt = SGD()\n",
    "    model.compile(optimizer=opt,\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'],\n",
    "                  \n",
    "                 )\n",
    "    #model.summary()\n",
    "\n",
    "    history = model.fit(train_X, train_y,\n",
    "                        epochs=epochs,\n",
    "                        batch_size=batch_size,\n",
    "                       validation_data=(val_X, val_y),\n",
    "                       verbose=False)\n",
    "\n",
    "    #plot_history(history)\n",
    "\n",
    "    loss, accuracy = model.evaluate(train_X, train_y, verbose=False)\n",
    "    print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "    \n",
    "    loss, accuracy = model.evaluate(test_X, test_y, verbose=False)\n",
    "    print(\"Testing Accuracy: {:.4f}\".format(accuracy))\n",
    "    #test_pred_y = (model.predict(test_X, batch_size=1024, verbose=1) > 0.5).astype(\"int32\")\n",
    "    #print(\"Report: \\n{}\".format(classification_report(test_y, test_pred_y)))\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Datos Originales\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se cargan los datos \n",
    "\n",
    "data = get_data_original_as_np_array()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Comienzo con una cantidad de parametros grande\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9977\n",
      "Testing Accuracy: 0.7748\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = run_model(data.x_train,data.y_train, data.x_val, data.y_val, data.x_test, data.y_test,\n",
    "                             embed_size = 300, max_features=20000, max_len = 50,\n",
    "                            batch_size = 512, nneuronsl1=16, activationl1='relu', epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Busco reducir la cantidad de parametros para reducir el overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 pruebo reduciendo max_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9955\n",
      "Testing Accuracy: 0.7925\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = run_model(data.x_train,data.y_train, data.x_val, data.y_val, data.x_test, data.y_test,\n",
    "                             embed_size = 100, max_features=12500, max_len = 50,\n",
    "                            batch_size = 512, nneuronsl1=16, activationl1='relu', epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9951\n",
      "Testing Accuracy: 0.7912\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = run_model(data.x_train,data.y_train, data.x_val, data.y_val, data.x_test, data.y_test,\n",
    "                             embed_size = 100, max_features=12500, max_len = 50,\n",
    "                            batch_size = 512, nneuronsl1=16, activationl1='relu', epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Pruebo reduciendo embed_size \n",
    "\n",
    "- Tamaño del vector de embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9953\n",
      "Testing Accuracy: 0.7853\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = run_model(data.x_train,data.y_train, data.x_val, data.y_val, data.x_test, data.y_test,\n",
    "                             embed_size = 100, max_features=12500, max_len = 50,\n",
    "                            batch_size = 512, nneuronsl1=16, activationl1='relu', epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9940\n",
      "Testing Accuracy: 0.7873\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = run_model(data.x_train,data.y_train, data.x_val, data.y_val, data.x_test, data.y_test,\n",
    "                             embed_size = 90, max_features=12500, max_len = 50,\n",
    "                            batch_size = 512, nneuronsl1=16, activationl1='relu', epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9949\n",
      "Testing Accuracy: 0.7754\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = run_model(data.x_train,data.y_train, data.x_val, data.y_val, data.x_test, data.y_test,\n",
    "                             embed_size = 80, max_features=12500, max_len = 50,\n",
    "                            batch_size = 512, nneuronsl1=16, activationl1='relu', epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9951\n",
      "Testing Accuracy: 0.7846\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = run_model(data.x_train,data.y_train, data.x_val, data.y_val, data.x_test, data.y_test,\n",
    "                             embed_size = 70, max_features=12500, max_len = 50,\n",
    "                            batch_size = 512, nneuronsl1=16, activationl1='relu', epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9945\n",
      "Testing Accuracy: 0.7787\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = run_model(data.x_train,data.y_train, data.x_val, data.y_val, data.x_test, data.y_test,\n",
    "                             embed_size = 60, max_features=12500, max_len = 50,\n",
    "                            batch_size = 512, nneuronsl1=16, activationl1='relu', epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Datos Preprocesados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se cargan los datos \n",
    "\n",
    "data = get_data_sentiment_analysis_as_np_array()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Comienzo con una cantidad de parametros grande"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9891\n",
      "Testing Accuracy: 0.7754\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = run_model(data.x_train,data.y_train, data.x_val, data.y_val, data.x_test, data.y_test,\n",
    "                             embed_size = 300, max_features=30000, max_len = 50,\n",
    "                            batch_size = 512, nneuronsl1=16, activationl1='relu', epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Busco reducir la cantidad de parametros para reducir el overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9871\n",
      "Testing Accuracy: 0.7058\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = run_model(data.x_train,data.y_train, data.x_val, data.y_val, data.x_test, data.y_test,\n",
    "                             embed_size = 300, max_features=13000, max_len = 50,\n",
    "                            batch_size = 512, nneuronsl1=16, activationl1='relu', epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9828\n",
      "Testing Accuracy: 0.7039\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = run_model(data.x_train,data.y_train, data.x_val, data.y_val, data.x_test, data.y_test,\n",
    "                             embed_size = 300, max_features=12000, max_len = 50,\n",
    "                            batch_size = 512, nneuronsl1=16, activationl1='relu', epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9893\n",
      "Testing Accuracy: 0.7846\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = run_model(data.x_train,data.y_train, data.x_val, data.y_val, data.x_test, data.y_test,\n",
    "                             embed_size = 300, max_features=12500, max_len = 50,\n",
    "                            batch_size = 512, nneuronsl1=16, activationl1='relu', epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9881\n",
      "Testing Accuracy: 0.7748\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = run_model(data.x_train,data.y_train, data.x_val, data.y_val, data.x_test, data.y_test,\n",
    "                             embed_size = 200, max_features=12500, max_len = 50,\n",
    "                            batch_size = 512, nneuronsl1=16, activationl1='relu', epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9887\n",
      "Testing Accuracy: 0.7748\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = run_model(data.x_train,data.y_train, data.x_val, data.y_val, data.x_test, data.y_test,\n",
    "                             embed_size = 100, max_features=12500, max_len = 50,\n",
    "                            batch_size = 512, nneuronsl1=16, activationl1='relu', epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9881\n",
      "Testing Accuracy: 0.7886\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = run_model(data.x_train,data.y_train, data.x_val, data.y_val, data.x_test, data.y_test,\n",
    "                             embed_size = 70, max_features=12500, max_len = 50,\n",
    "                            batch_size = 512, nneuronsl1=16, activationl1='relu', epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "pred_X = tokenizer.texts_to_sequences(data.predict)\n",
    "pred_X = pad_sequences(pred_X, maxlen=50)\n",
    "\n",
    "y_pred = (model.predict(pred_X, batch_size=512, verbose=1) > 0.5).astype(\"int32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv(os.path.join('..','0_Data','dataset','sample_submission.csv'))\n",
    "submission['target'] = y_pred\n",
    "submission.to_csv(\"submission8.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
