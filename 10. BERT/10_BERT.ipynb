{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --ignore-installed tensorflow-gpu --user\n",
    "# !pip install bert-for-tf2\n",
    "# !pip install --ignore-installed graphviz\n",
    "# !pip install --ignore-installed pydot\n",
    "# !pip install pydotplus\n",
    "# !pip install tf-models-nightly\n",
    "# !pip uninstall tf-nightly\n",
    "# !pip uninstall tf-estimator-nightly\n",
    "# !pip install kaggle\n",
    "# !pip install keras_lr_finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow import keras\n",
    "import os\n",
    "import bert\n",
    "from bert import BertModelLayer\n",
    "from bert.loader import StockBertConfig, map_stock_config_to_params, load_stock_weights\n",
    "from bert.tokenization.bert_tokenization import FullTokenizer\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import kaggle\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.utils import compute_class_weight\n",
    "from keras_lr_finder import LRFinder\n",
    "\n",
    "\n",
    "plt.rc('figure', figsize=(20,10))\n",
    "plt.rc('axes', labelsize=18, titlesize=20, titleweight = 'bold')    # tama単o de label y titulo \n",
    "plt.rc('xtick', labelsize=14)    # tama単o de los indicadores de variacion eje x\n",
    "plt.rc('ytick', labelsize=14)    # tama単o de los indicadores de variacion eje y\n",
    "plt.rc('legend', fontsize=14)    # tama単o del indicador (por ej, verdadero o falso)\n",
    "\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "  except RuntimeError as e:\n",
    "    print(e)\n",
    "\n",
    "%run ../0_Data/0_DataLoader.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hiperparametros y parametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adapter_size = None #64\n",
    "learning_rate = 1e-5\n",
    "max_seq_len = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tweets:\n",
    "    DATA_COLUMN = \"text\"\n",
    "    LABEL_COLUMN = \"target\"\n",
    "\n",
    "    def __init__(self, tokenizer: FullTokenizer, sample_size=None, max_seq_len=1024, train=None, test=None):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.sample_size = sample_size\n",
    "        self.max_seq_len = 0\n",
    "        self.train = train\n",
    "        self.test = test\n",
    "        train = train[['text', 'target']].reset_index(drop=True)\n",
    "        test = test[['text', 'target']].reset_index(drop=True)\n",
    "        \n",
    "        ((self.train_x, self.train_y),\n",
    "         (self.test_x, self.test_y)) = map(self._prepare, [train, test])\n",
    "        \n",
    "        print(\"max seq_len\", self.max_seq_len)\n",
    "        self.max_seq_len = min(self.max_seq_len, max_seq_len)\n",
    "        ((self.train_x, self.train_x_token_types),\n",
    "         (self.test_x, self.test_x_token_types)) = map(self._pad, [self.train_x, self.test_x])\n",
    "\n",
    "    def _prepare(self, df):\n",
    "        x, y = [], []\n",
    "        with tqdm(total=df.shape[0], unit_scale=True) as pbar:\n",
    "            for ndx, row in df.iterrows():\n",
    "                text, label = row[Tweets.DATA_COLUMN], row[Tweets.LABEL_COLUMN]\n",
    "                tokens = self.tokenizer.tokenize(text)\n",
    "                tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n",
    "                token_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
    "                self.max_seq_len = max(self.max_seq_len, len(token_ids))\n",
    "                x.append(token_ids)\n",
    "                y.append(int(label))\n",
    "                pbar.update()\n",
    "        return np.array(x), np.array(y)\n",
    "    \n",
    "    def _pad(self, ids):\n",
    "        x, t = [], []\n",
    "        token_type_ids = [0] * self.max_seq_len\n",
    "        for input_ids in ids:\n",
    "            input_ids = input_ids[:min(len(input_ids), self.max_seq_len - 2)]\n",
    "            input_ids = input_ids + [0] * (self.max_seq_len - len(input_ids))\n",
    "            x.append(np.array(input_ids))\n",
    "            t.append(token_type_ids)\n",
    "        return np.array(x), np.array(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_ckpt_dir=\".models/uncased_L-12_H-768_A-12/\"\n",
    "bert_ckpt_file = bert_ckpt_dir + \"bert_model.ckpt\"\n",
    "bert_config_file = bert_ckpt_dir + \"bert_config.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_layers(root_layer):\n",
    "    if isinstance(root_layer, keras.layers.Layer):\n",
    "        yield root_layer\n",
    "    for layer in root_layer._layers:\n",
    "        for sub_layer in flatten_layers(layer):\n",
    "            yield sub_layer\n",
    "\n",
    "\n",
    "def freeze_bert_layers(l_bert):\n",
    "    \"\"\"\n",
    "    Freezes all but LayerNorm and adapter layers - see arXiv:1902.00751.\n",
    "    \"\"\"\n",
    "    for layer in flatten_layers(l_bert):\n",
    "        if layer.name in [\"LayerNorm\", \"adapter-down\", \"adapter-up\"]:\n",
    "            layer.trainable = True\n",
    "        elif len(layer._layers) == 0:\n",
    "            layer.trainable = False\n",
    "        l_bert.embeddings_layer.trainable = False\n",
    "\n",
    "\n",
    "def create_learning_rate_scheduler(max_learn_rate=5e-5,\n",
    "                                   end_learn_rate=1e-7,\n",
    "                                   warmup_epoch_count=10,\n",
    "                                   total_epoch_count=90):\n",
    "\n",
    "    def lr_scheduler(epoch):\n",
    "        if epoch < warmup_epoch_count:\n",
    "            res = (max_learn_rate/warmup_epoch_count) * (epoch + 1)\n",
    "        else:\n",
    "            res = max_learn_rate*math.exp(math.log(end_learn_rate/max_learn_rate)*(epoch-warmup_epoch_count+1)/(total_epoch_count-warmup_epoch_count+1))\n",
    "        return float(res)\n",
    "    learning_rate_scheduler = tf.keras.callbacks.LearningRateScheduler(lr_scheduler, verbose=1)\n",
    "\n",
    "    return learning_rate_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(max_seq_len, adapter_size=64):\n",
    "  # create the bert layer\n",
    "  with tf.io.gfile.GFile(bert_config_file, \"r\") as reader:\n",
    "      bc = StockBertConfig.from_json_string(reader.read())\n",
    "      bert_params = map_stock_config_to_params(bc)\n",
    "      bert_params.adapter_size = adapter_size\n",
    "      bert = BertModelLayer.from_params(bert_params, name=\"bert\")\n",
    "        \n",
    "  input_ids      = keras.layers.Input(shape=(max_seq_len,), dtype='int32', name=\"input_ids\")\n",
    "  output         = bert(input_ids)\n",
    "\n",
    "  print(\"bert shape\", output.shape)\n",
    "  cls_out = keras.layers.Lambda(lambda seq: seq[:, 0, :])(output)\n",
    "  logits = keras.layers.Dense(units=1, activation=None)(cls_out)\n",
    "\n",
    "  model = keras.Model(inputs=input_ids, outputs=logits)\n",
    "  model.build(input_shape=(None, max_seq_len))\n",
    "\n",
    "  # load the pre-trained model weights\n",
    "  load_stock_weights(bert, bert_ckpt_file)\n",
    "\n",
    "  if adapter_size is not None:\n",
    "      freeze_bert_layers(bert)\n",
    "\n",
    "  model.compile(optimizer=keras.optimizers.Adam(learning_rate=2e-5),\n",
    "                loss=keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                metrics=[keras.metrics.BinaryAccuracy(name=\"acc\")])\n",
    "\n",
    "  model.summary()\n",
    "        \n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "tokenizer = FullTokenizer(vocab_file=os.path.join(bert_ckpt_dir, \"vocab.txt\"))\n",
    "\n",
    "kf = StratifiedKFold(n_splits=5, random_state=1, shuffle=True)\n",
    "\n",
    "\n",
    "results = []\n",
    "history = 0\n",
    "def entrenar(train_df, val_df):\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "    tf.compat.v1.reset_default_graph()\n",
    "    gc.collect()\n",
    "\n",
    "    global max_seq_len\n",
    "\n",
    "    #Lo hacemos 2 veces, la primera con el dataframe de test para que nos tire el max_seq_len y la otra\n",
    "    # para que formatee bien los datos\n",
    "    data = Tweets(tokenizer, sample_size=None, max_seq_len=max_seq_len, train=train_df, test=train_df)    \n",
    "\n",
    "    max_seq_len = data.max_seq_len\n",
    "    \n",
    "    data = Tweets(tokenizer, sample_size=None, max_seq_len=max_seq_len, train=train_df, test=val_df)    \n",
    "    \n",
    "    model = create_model(max_seq_len, adapter_size=adapter_size)\n",
    "    \n",
    "    # model is a Keras model\n",
    "    lr_finder = LRFinder(model)\n",
    "    \n",
    "    # Train a model with batch size 512 for 5 epochs\n",
    "    # with learning rate growing exponentially from 0.0001 to 1\n",
    "#     lr_finder.find(data.train_x, data.train_y, start_lr=5.14e-6, end_lr=5.16e-6, batch_size=16, epochs=10)\n",
    "    # Plot the loss, ignore 20 batches in the beginning and 5 in the end\n",
    "#     lr_finder.plot_loss()\n",
    "    \n",
    "    log_dir = \".log/real_or_not/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    tensorboard_callback = keras.callbacks.TensorBoard(log_dir=log_dir)\n",
    "\n",
    "    class_weights = compute_class_weight('balanced', np.unique(train_df.target), train_df.target) \n",
    "    class_weights_dict = dict(enumerate(class_weights))\n",
    "\n",
    "    history = model.fit(x=data.train_x, y=data.train_y, batch_size=32, epochs=2,\n",
    "                validation_data=(data.test_x, data.test_y),\n",
    "#               class_weight = class_weights_dict,                        \n",
    "  callbacks=[tensorboard_callback, keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)])\n",
    "\n",
    "# create_learning_rate_scheduler(max_learn_rate=2e-5,\n",
    "#                                                         end_learn_rate=1e-5,\n",
    "#                                                         warmup_epoch_count=20,\n",
    "#                                                         total_epoch_count=total_epoch_count),\n",
    "    \n",
    "    _, train_acc = model.evaluate(data.train_x, data.train_y)\n",
    "    _, test_acc = model.evaluate(data.test_x , data.test_y)\n",
    "    print(\"train acc\", train_acc)\n",
    "    print(\" test acc\", test_acc)\n",
    "    results.append(test_acc)\n",
    "    model.save_weights('./real_or_not.h5', overwrite=True)\n",
    "    tf.keras.backend.clear_session()\n",
    "    tf.compat.v1.reset_default_graph()\n",
    "    gc.collect()\n",
    "\n",
    "datos = get_data_sentiment_analysis_as_database()\n",
    "\n",
    "test_data = datos.test\n",
    "train_df = datos.train\n",
    "val_data = datos.validation\n",
    "predict = datos.predict\n",
    "\n",
    "entrenar(train_df, val_data)\n",
    "\n",
    "# split, df_train, test_data = get_k_folded_data_original_as_database()\n",
    "\n",
    "# #Mantenemos los pesos de cada clase en cada K-Fold\n",
    "# for train_index, val_index in split:\n",
    "#     train_df = df_train.iloc[train_index]\n",
    "#     val_df = df_train.iloc[val_index]\n",
    "#     entrenar(train_df, val_df)\n",
    "\n",
    "    \n",
    "print(\"results\",results)\n",
    "print(f\"Mean-Precision: {sum(results) / len(results)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "# model.save_weights('./real_or_not.h5', overwrite=True)\n",
    "tokenizer = FullTokenizer(vocab_file=os.path.join(bert_ckpt_dir, \"vocab.txt\"))\n",
    "\n",
    "\n",
    "data = Tweets(tokenizer, sample_size=None, max_seq_len=max_seq_len, train=train_df, test=test_data)    \n",
    "model = create_model(max_seq_len, adapter_size=adapter_size)\n",
    "model.load_weights(\"real_or_not.h5\")\n",
    "\n",
    "\n",
    "_, test_acc = model.evaluate(data.test_x, data.test_y)\n",
    "\n",
    "print(\"train acc\", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    acc = history.history['acc']\n",
    "    loss = history.history['loss']\n",
    "    x = range(1, len(acc) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.plot(x, acc, 'r', label='Training acc')\n",
    "    plt.plot(x, loss, 'b', label='Training loss')\n",
    "    plt.title('Training accuracy vs loss')\n",
    "    plt.legend()\n",
    "    \n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "predict = datos.predict\n",
    "\n",
    "max_seq_len1 = 0\n",
    "\n",
    "x, y = [], []\n",
    "for ndx, row in predict.iterrows():\n",
    "    text = row['text']\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    max_seq_len1 = max(max_seq_len1, len(token_ids))\n",
    "    x.append(token_ids)\n",
    "ids = np.array(x)\n",
    "max_seq_len1 = min(max_seq_len1, max_seq_len)\n",
    "\n",
    "x, t = [], []\n",
    "token_type_ids = [0] * max_seq_len1\n",
    "for input_ids in ids:\n",
    "    input_ids = input_ids[:min(len(input_ids), max_seq_len1 - 2)]\n",
    "    input_ids = input_ids + [0] * (max_seq_len1 - len(input_ids))\n",
    "    x.append(np.array(input_ids))\n",
    "    t.append(token_type_ids)\n",
    "test_x, test_x_token_types = np.array(x), np.array(t)\n",
    "\n",
    "model = create_model(max_seq_len, adapter_size=adapter_size)\n",
    "model.load_weights(\"real_or_not.h5\")\n",
    "\n",
    "y_test = (model.predict(test_x, batch_size=16, verbose=1) > 0.5).astype(\"int32\")\n",
    "submission = pd.read_csv('../dataset/sample_submission.csv')\n",
    "submission['target'] = y_test\n",
    "submission.to_csv(\"submission3.csv\", index=False)\n",
    "\n",
    "# !kaggle competitions submit nlp-getting-started -f submission3.csv -m 'BERT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.target.value_counts()/len(test_data.target)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
