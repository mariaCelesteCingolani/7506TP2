{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow-gpu\n",
    "# !pip install bert-for-tf2\n",
    "# !pip install --ignore-installed graphviz\n",
    "# !pip install --ignore-installed pydot\n",
    "# !pip install pydotplus\n",
    "# !pip install kaggle\n",
    "# !pip install keras_lr_finder\n",
    "# !pip install tf-models-nightly\n",
    "# !pip uninstall tf-nightly\n",
    "# !pip uninstall tf-estimator-nightly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 6096), started 4:13:52 ago. (Use '!kill 6096' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-5151d9e142a53af1\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-5151d9e142a53af1\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\germa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\germa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\germa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import bert\n",
    "from bert import BertModelLayer\n",
    "from bert.loader import StockBertConfig, map_stock_config_to_params, load_stock_weights\n",
    "from bert.tokenization.bert_tokenization import FullTokenizer\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import kaggle\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.utils import compute_class_weight\n",
    "from keras_lr_finder import LRFinder\n",
    "import random\n",
    "\n",
    "logs_base_dir = \".\\logs\"\n",
    "os.makedirs(logs_base_dir, exist_ok=True)\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir={logs_base_dir}\n",
    "\n",
    "# Hacemos que el modelo sea 100% deterministico incluso corriendo con aceleracion GPU\n",
    "SEED = 12\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "os.environ['PYTHONHASHSEED']=str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "plt.rc('figure', figsize=(20,10))\n",
    "plt.rc('axes', labelsize=18, titlesize=20, titleweight = 'bold')    # tamaño de label y titulo \n",
    "plt.rc('xtick', labelsize=14)    # tamaño de los indicadores de variacion eje x\n",
    "plt.rc('ytick', labelsize=14)    # tamaño de los indicadores de variacion eje y\n",
    "plt.rc('legend', fontsize=14)    # tamaño del indicador (por ej, verdadero o falso)\n",
    "\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR) \n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "  except RuntimeError as e:\n",
    "    print(e)\n",
    "\n",
    "%run ../0_Data/0_DataLoader.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Hiperparametros y parametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "adapter_size = None #64\n",
    "learning_rate = 0.00001\n",
    "max_seq_len = 96\n",
    "batch_size = 24\n",
    "epochs = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Clase para tokenizar los inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tweets:\n",
    "    DATA_COLUMN = \"text\"\n",
    "    LABEL_COLUMN = \"target\"\n",
    "\n",
    "    def __init__(self, tokenizer: FullTokenizer, max_seq_len=1024, train=None, test=None):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_seq_len = 0\n",
    "        self.train = train\n",
    "        self.test = test\n",
    "        train = train[['text', 'target']].reset_index(drop=True)\n",
    "        test = test[['text', 'target']].reset_index(drop=True)\n",
    "        \n",
    "        ((self.train_x, self.train_y),\n",
    "         (self.test_x, self.test_y)) = map(self._prepare, [train, test])\n",
    "        \n",
    "        print(\"max seq_len\", self.max_seq_len)\n",
    "#         self.max_seq_len = min(self.max_seq_len, max_seq_len)\n",
    "        self.max_seq_len = max_seq_len\n",
    "        ((self.train_x, self.train_x_token_types),\n",
    "         (self.test_x, self.test_x_token_types)) = map(self._pad, [self.train_x, self.test_x])\n",
    "\n",
    "    def _prepare(self, df):\n",
    "        x, y = [], []\n",
    "        with tqdm(total=df.shape[0], unit_scale=True) as pbar:\n",
    "            for ndx, row in df.iterrows():\n",
    "                text, label = row[Tweets.DATA_COLUMN], row[Tweets.LABEL_COLUMN]\n",
    "                tokens = self.tokenizer.tokenize(text)\n",
    "                tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n",
    "                token_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
    "#                 self.max_seq_len = max(self.max_seq_len, len(token_ids))\n",
    "                x.append(token_ids)\n",
    "                y.append(int(label))\n",
    "                pbar.update()\n",
    "        return np.array(x), np.array(y)\n",
    "    \n",
    "    def _pad(self, ids):\n",
    "        x, t = [], []\n",
    "        token_type_ids = [0] * self.max_seq_len\n",
    "        for input_ids in ids:\n",
    "            input_ids = input_ids[:min(len(input_ids), self.max_seq_len - 2)]\n",
    "            input_ids = input_ids + [0] * (self.max_seq_len - len(input_ids))\n",
    "            x.append(np.array(input_ids))\n",
    "            t.append(token_type_ids)\n",
    "        return np.array(x), np.array(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Funciones para compilar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_ckpt_dir=\".models/uncased_L-12_H-768_A-12/\"\n",
    "bert_ckpt_file = bert_ckpt_dir + \"bert_model.ckpt\"\n",
    "bert_config_file = bert_ckpt_dir + \"bert_config.json\"\n",
    "\n",
    "def flatten_layers(root_layer):\n",
    "    if isinstance(root_layer, tf.keras.layers.Layer):\n",
    "        yield root_layer\n",
    "    for layer in root_layer._layers:\n",
    "        for sub_layer in flatten_layers(layer):\n",
    "            yield sub_layer\n",
    "\n",
    "def freeze_bert_layers(l_bert):\n",
    "    \"\"\"\n",
    "    Freezes all but LayerNorm and adapter layers - see arXiv:1902.00751.\n",
    "    \"\"\"\n",
    "    for layer in flatten_layers(l_bert):\n",
    "        if layer.name in [\"LayerNorm\", \"adapter-down\", \"adapter-up\"]:\n",
    "            layer.trainable = True\n",
    "        elif len(layer._layers) == 0:\n",
    "            layer.trainable = False\n",
    "        l_bert.embeddings_layer.trainable = False\n",
    "\n",
    "\n",
    "def create_learning_rate_scheduler(max_learn_rate=5e-5,\n",
    "                                   end_learn_rate=1e-7,\n",
    "                                   warmup_epoch_count=10,\n",
    "                                   total_epoch_count=90):\n",
    "\n",
    "    def lr_scheduler(epoch):\n",
    "        if epoch < warmup_epoch_count:\n",
    "            res = (max_learn_rate/warmup_epoch_count) * (epoch + 1)\n",
    "        else:\n",
    "            res = max_learn_rate*math.exp(math.log(end_learn_rate/max_learn_rate)*(epoch-warmup_epoch_count+1)/(total_epoch_count-warmup_epoch_count+1))\n",
    "        return float(res)\n",
    "    learning_rate_scheduler = tf.keras.callbacks.LearningRateScheduler(lr_scheduler, verbose=1)\n",
    "\n",
    "    return learning_rate_scheduler\n",
    "\n",
    "def create_model(max_seq_len, adapter_size=64):\n",
    "  # create the bert layer\n",
    "  with tf.io.gfile.GFile(bert_config_file, \"r\") as reader:\n",
    "      bc = StockBertConfig.from_json_string(reader.read())\n",
    "      bert_params = map_stock_config_to_params(bc)\n",
    "      bert_params.adapter_size = adapter_size\n",
    "      bert = BertModelLayer.from_params(bert_params, name=\"bert\")\n",
    "        \n",
    "  input_ids      = tf.keras.layers.Input(shape=(max_seq_len,), dtype='int32', name=\"input_ids\")\n",
    "  output         = bert(input_ids)\n",
    "\n",
    "  print(\"bert shape\", output.shape)\n",
    "  cls_out = tf.keras.layers.Lambda(lambda seq: seq[:, 0, :])(output)\n",
    "#   cls_out1 = tf.keras.layers.Dropout(0.5)(cls_out)\n",
    "#   logits0 = tf.keras.layers.Dense(units=768, activation=\"tanh\")(cls_out1)\n",
    "  middle = tf.keras.layers.Dropout(0.5)(cls_out)\n",
    "  logits = tf.keras.layers.Dense(units=1, activation=None)(middle)\n",
    "\n",
    "  model = tf.keras.Model(inputs=input_ids, outputs=logits)\n",
    "  model.build(input_shape=(None, max_seq_len))\n",
    "\n",
    "  # load the pre-trained model weights\n",
    "  load_stock_weights(bert, bert_ckpt_file)\n",
    "\n",
    "  if adapter_size is not None:\n",
    "      freeze_bert_layers(bert)\n",
    "\n",
    "  model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),#SGD(learning_rate=learning_rate, momentum=0.9, nesterov=True),\n",
    "                loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                metrics=[tf.keras.metrics.BinaryAccuracy()])\n",
    "\n",
    "  model.summary()\n",
    "        \n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Entrenamiento y validación del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4.87k/4.87k [00:02<00:00, 1.77kit/s]\n",
      "100%|██████████| 1.22k/1.22k [00:00<00:00, 1.81kit/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max seq_len 0\n",
      "bert shape (None, 96, 768)\n",
      "Done loading 196 BERT weights from: .models/uncased_L-12_H-768_A-12/bert_model.ckpt into <bert.model.BertModelLayer object at 0x000001F328E992E0> (prefix:bert). Count of weights not found in the checkpoint was: [0]. Count of weights with mismatched shape: [0]\n",
      "Unused weights from checkpoint: \n",
      "\tbert/embeddings/token_type_embeddings\n",
      "\tbert/pooler/dense/bias\n",
      "\tbert/pooler/dense/kernel\n",
      "\tcls/predictions/output_bias\n",
      "\tcls/predictions/transform/LayerNorm/beta\n",
      "\tcls/predictions/transform/LayerNorm/gamma\n",
      "\tcls/predictions/transform/dense/bias\n",
      "\tcls/predictions/transform/dense/kernel\n",
      "\tcls/seq_relationship/output_bias\n",
      "\tcls/seq_relationship/output_weights\n",
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_ids (InputLayer)       [(None, 96)]              0         \n",
      "_________________________________________________________________\n",
      "bert (BertModelLayer)        (None, 96, 768)           108890112 \n",
      "_________________________________________________________________\n",
      "lambda (Lambda)              (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 769       \n",
      "=================================================================\n",
      "Total params: 108,890,881\n",
      "Trainable params: 108,890,881\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\germa\\documents\\proyectos python\\7506tp2\\venv\\lib\\site-packages\\sklearn\\utils\\validation.py:68: FutureWarning: Pass classes=[0 1], y=id\n",
      "3774    0\n",
      "203     1\n",
      "4778    1\n",
      "3879    1\n",
      "4692    1\n",
      "       ..\n",
      "1607    0\n",
      "3745    1\n",
      "5002    0\n",
      "1726    0\n",
      "3933    1\n",
      "Name: target, Length: 4872, dtype: int64 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "203/203 [==============================] - 198s 975ms/step - loss: 0.5902 - binary_accuracy: 0.7190 - val_loss: 0.5595 - val_binary_accuracy: 0.7816\n",
      "Epoch 2/3\n",
      "203/203 [==============================] - 194s 957ms/step - loss: 0.4388 - binary_accuracy: 0.8245 - val_loss: 0.4493 - val_binary_accuracy: 0.8030\n",
      "Epoch 3/3\n",
      "203/203 [==============================] - 193s 949ms/step - loss: 0.3795 - binary_accuracy: 0.8557 - val_loss: 0.4780 - val_binary_accuracy: 0.8103\n",
      "153/153 [==============================] - 51s 332ms/step - loss: 0.3002 - binary_accuracy: 0.8879\n",
      "39/39 [==============================] - 13s 322ms/step - loss: 0.4493 - binary_accuracy: 0.8030\n",
      "train acc 0.8879310488700867\n",
      " test acc 0.802955687046051\n",
      "results [0.802955687046051]\n",
      "Mean-Precision: 0.802955687046051\n",
      "Wall time: 11min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "tokenizer = FullTokenizer(vocab_file=os.path.join(bert_ckpt_dir, \"vocab.txt\"))\n",
    "\n",
    "results = []\n",
    "history = 0\n",
    "def entrenar(train_df, val_df):\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "    tf.compat.v1.reset_default_graph()\n",
    "    gc.collect()\n",
    "\n",
    "    global max_seq_len\n",
    "    \n",
    "    data = Tweets(tokenizer, max_seq_len=max_seq_len, train=train_df, test=val_df)    \n",
    "    \n",
    "    model = create_model(max_seq_len, adapter_size=adapter_size)\n",
    "        \n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logs_base_dir)\n",
    "\n",
    "    class_weights = compute_class_weight('balanced', np.unique(train_df.target), train_df.target) \n",
    "    class_weights_dict = dict(enumerate(class_weights))\n",
    "    \n",
    "    history = model.fit(x=data.train_x, y=data.train_y, batch_size=batch_size, epochs=epochs,\n",
    "                validation_data=(data.test_x, data.test_y),\n",
    "                class_weight = class_weights_dict,                        \n",
    "                callbacks=[tensorboard_callback, \n",
    "                           tf.keras.callbacks.EarlyStopping(patience=1, restore_best_weights=True)])\n",
    "\n",
    "#           callbacks=[create_learning_rate_scheduler(max_learn_rate=1e-5,\n",
    "#                                                     end_learn_rate=1e-7,\n",
    "#                                                     warmup_epoch_count=20,\n",
    "#                                                     total_epoch_count=epochs),\n",
    "#                      tf.keras.callbacks.EarlyStopping(patience=20, restore_best_weights=True),\n",
    "#                      tensorboard_callback])\n",
    "    \n",
    "    _, train_acc = model.evaluate(data.train_x, data.train_y)\n",
    "    _, test_acc = model.evaluate(data.test_x , data.test_y)\n",
    "    print(\"train acc\", train_acc)\n",
    "    print(\" test acc\", test_acc)\n",
    "    results.append(test_acc)\n",
    "    model.save_weights('./real_or_not.h5', overwrite=True)\n",
    "    tf.keras.backend.clear_session()\n",
    "    tf.compat.v1.reset_default_graph()\n",
    "    gc.collect()\n",
    "\n",
    "datos = get_data_original_as_database()\n",
    "\n",
    "test_data = datos.test\n",
    "train_df = datos.train\n",
    "val_data = datos.validation\n",
    "predict = datos.predict\n",
    "\n",
    "entrenar(train_df, val_data)\n",
    "\n",
    "# split, df_train, test_data = get_k_folded_data_original_as_database()\n",
    "\n",
    "# #Mantenemos los pesos de cada clase en cada K-Fold\n",
    "# for train_index, val_index in split:\n",
    "#     train_df = df_train.iloc[train_index]\n",
    "#     val_df = df_train.iloc[val_index]\n",
    "#     entrenar(train_df, val_df)\n",
    "\n",
    "    \n",
    "print(\"results\",results)\n",
    "print(f\"Mean-Precision: {sum(results) / len(results)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Testeo del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4.87k/4.87k [00:02<00:00, 1.75kit/s]\n",
      "100%|██████████| 1.52k/1.52k [00:00<00:00, 1.76kit/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max seq_len 0\n",
      "bert shape (None, 96, 768)\n",
      "Done loading 196 BERT weights from: .models/uncased_L-12_H-768_A-12/bert_model.ckpt into <bert.model.BertModelLayer object at 0x000001F37ACDC3D0> (prefix:bert). Count of weights not found in the checkpoint was: [0]. Count of weights with mismatched shape: [0]\n",
      "Unused weights from checkpoint: \n",
      "\tbert/embeddings/token_type_embeddings\n",
      "\tbert/pooler/dense/bias\n",
      "\tbert/pooler/dense/kernel\n",
      "\tcls/predictions/output_bias\n",
      "\tcls/predictions/transform/LayerNorm/beta\n",
      "\tcls/predictions/transform/LayerNorm/gamma\n",
      "\tcls/predictions/transform/dense/bias\n",
      "\tcls/predictions/transform/dense/kernel\n",
      "\tcls/seq_relationship/output_bias\n",
      "\tcls/seq_relationship/output_weights\n",
      "Model: \"functional_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_ids (InputLayer)       [(None, 96)]              0         \n",
      "_________________________________________________________________\n",
      "bert (BertModelLayer)        (None, 96, 768)           108890112 \n",
      "_________________________________________________________________\n",
      "lambda_1 (Lambda)            (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 769       \n",
      "=================================================================\n",
      "Total params: 108,890,881\n",
      "Trainable params: 108,890,881\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "48/48 [==============================] - 16s 326ms/step - loss: 0.3995 - binary_accuracy: 0.8385\n",
      "train acc 0.8384767174720764\n",
      "Wall time: 32.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "tokenizer = FullTokenizer(vocab_file=os.path.join(bert_ckpt_dir, \"vocab.txt\"))\n",
    "\n",
    "data = Tweets(tokenizer, max_seq_len=max_seq_len, train=train_df, test=test_data)    \n",
    "model = create_model(max_seq_len, adapter_size=adapter_size)\n",
    "model.load_weights(\"real_or_not.h5\")\n",
    "\n",
    "\n",
    "_, test_acc = model.evaluate(data.test_x, data.test_y)\n",
    "\n",
    "print(\"train acc\", test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Predecimos los datos y submiteamos a Kaggle"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x, y = [], []\n",
    "for ndx, row in predict.iterrows():\n",
    "    text = row['text']\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    x.append(token_ids)\n",
    "ids = np.array(x)\n",
    "\n",
    "x, t = [], []\n",
    "token_type_ids = [0] * max_seq_len\n",
    "for input_ids in ids:\n",
    "    input_ids = input_ids[:min(len(input_ids), max_seq_len - 2)]\n",
    "    input_ids = input_ids + [0] * (max_seq_len - len(input_ids))\n",
    "    x.append(np.array(input_ids))\n",
    "    t.append(token_type_ids)\n",
    "test_x, test_x_token_types = np.array(x), np.array(t)\n",
    "\n",
    "model = create_model(max_seq_len, adapter_size=adapter_size)\n",
    "model.load_weights(\"real_or_not.h5\")\n",
    "\n",
    "y_test = (model.predict(test_x, batch_size=batch_size, verbose=1) > 0.5).astype(\"int32\")\n",
    "submission = pd.read_csv('../dataset/sample_submission.csv')\n",
    "submission['target'] = y_test\n",
    "submission.to_csv(\"submission3.csv\", index=False)\n",
    "\n",
    "!kaggle competitions submit nlp-getting-started -f submission3.csv -m 'BERT'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x, y = [], []\n",
    "for ndx, row in predict.iterrows():\n",
    "    text = row['text']\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    x.append(token_ids)\n",
    "ids = np.array(x)\n",
    "\n",
    "x, t = [], []\n",
    "token_type_ids = [0] * max_seq_len\n",
    "for input_ids in ids:\n",
    "    input_ids = input_ids[:min(len(input_ids), max_seq_len - 2)]\n",
    "    input_ids = input_ids + [0] * (max_seq_len - len(input_ids))\n",
    "    x.append(np.array(input_ids))\n",
    "    t.append(token_type_ids)\n",
    "test_x, test_x_token_types = np.array(x), np.array(t)\n",
    "\n",
    "model = create_model(max_seq_len, adapter_size=adapter_size)\n",
    "model.load_weights(\"real_or_not.h5\")\n",
    "\n",
    "y_test = (model.predict(test_x, batch_size=batch_size, verbose=1) > 0.5).astype(\"int32\")\n",
    "submission = pd.read_csv('../dataset/sample_submission.csv')\n",
    "submission['target'] = y_test\n",
    "submission.to_csv(\"submission3.csv\", index=False)\n",
    "\n",
    "!kaggle competitions submit nlp-getting-started -f submission3.csv -m 'BERT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert shape (None, 96, 768)\n",
      "Done loading 196 BERT weights from: .models/uncased_L-12_H-768_A-12/bert_model.ckpt into <bert.model.BertModelLayer object at 0x00000287248611F0> (prefix:bert). Count of weights not found in the checkpoint was: [0]. Count of weights with mismatched shape: [0]\n",
      "Unused weights from checkpoint: \n",
      "\tbert/embeddings/token_type_embeddings\n",
      "\tbert/pooler/dense/bias\n",
      "\tbert/pooler/dense/kernel\n",
      "\tcls/predictions/output_bias\n",
      "\tcls/predictions/transform/LayerNorm/beta\n",
      "\tcls/predictions/transform/LayerNorm/gamma\n",
      "\tcls/predictions/transform/dense/bias\n",
      "\tcls/predictions/transform/dense/kernel\n",
      "\tcls/seq_relationship/output_bias\n",
      "\tcls/seq_relationship/output_weights\n",
      "Model: \"functional_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_ids (InputLayer)       [(None, 96)]              0         \n",
      "_________________________________________________________________\n",
      "bert (BertModelLayer)        (None, 96, 768)           108890112 \n",
      "_________________________________________________________________\n",
      "lambda_1 (Lambda)            (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 769       \n",
      "=================================================================\n",
      "Total params: 108,890,881\n",
      "Trainable params: 108,890,881\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "136/136 [==============================] - 34s 249ms/step\n",
      "Successfully submitted to Real or Not? NLP with Disaster Tweets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0.00/25.4k [00:00<?, ?B/s]\n",
      " 31%|###1      | 8.00k/25.4k [00:00<00:00, 51.6kB/s]\n",
      "100%|##########| 25.4k/25.4k [00:02<00:00, 10.1kB/s]\n"
     ]
    }
   ],
   "source": [
    "x, y = [], []\n",
    "for ndx, row in predict.iterrows():\n",
    "    text = row['text']\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    x.append(token_ids)\n",
    "ids = np.array(x)\n",
    "\n",
    "x, t = [], []\n",
    "token_type_ids = [0] * max_seq_len\n",
    "for input_ids in ids:\n",
    "    input_ids = input_ids[:min(len(input_ids), max_seq_len - 2)]\n",
    "    input_ids = input_ids + [0] * (max_seq_len - len(input_ids))\n",
    "    x.append(np.array(input_ids))\n",
    "    t.append(token_type_ids)\n",
    "test_x, test_x_token_types = np.array(x), np.array(t)\n",
    "\n",
    "model = create_model(max_seq_len, adapter_size=adapter_size)\n",
    "model.load_weights(\"real_or_not.h5\")\n",
    "\n",
    "y_test = (model.predict(test_x, batch_size=batch_size, verbose=1) > 0.5).astype(\"int32\")\n",
    "submission = pd.read_csv('../dataset/sample_submission.csv')\n",
    "submission['target'] = y_test\n",
    "submission.to_csv(\"submission3.csv\", index=False)\n",
    "\n",
    "!kaggle competitions submit nlp-getting-started -f submission3.csv -m 'BERT'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}