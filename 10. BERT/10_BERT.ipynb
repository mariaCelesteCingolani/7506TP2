{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow-gpu\n",
    "# !pip install bert-for-tf2\n",
    "# !pip install --ignore-installed graphviz\n",
    "# !pip install --ignore-installed pydot\n",
    "# !pip install pydotplus\n",
    "# !pip install kaggle\n",
    "# !pip install keras_lr_finder\n",
    "# !pip install tensorflow-addons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\germa\\documents\\proyectos python\\7506tp2\\venv\\lib\\site-packages\\tensorflow_addons\\utils\\ensure_tf_install.py:54: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.2.0 and strictly below 2.3.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.3.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 6096), started 1 day, 2:36:33 ago. (Use '!kill 6096' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-57ad845d13b13734\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-57ad845d13b13734\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\germa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\germa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\germa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import bert\n",
    "from bert import BertModelLayer\n",
    "from bert.loader import StockBertConfig, map_stock_config_to_params, load_stock_weights\n",
    "from bert.tokenization.bert_tokenization import FullTokenizer\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import kaggle\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.utils import compute_class_weight\n",
    "from keras_lr_finder import LRFinder\n",
    "import tensorflow_addons as tfa\n",
    "import random\n",
    "\n",
    "logs_base_dir = \".\\logs\"\n",
    "os.makedirs(logs_base_dir, exist_ok=True)\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir={logs_base_dir}\n",
    "\n",
    "# Hacemos que el modelo sea 100% deterministico incluso corriendo con aceleracion GPU\n",
    "SEED = 12\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "os.environ['PYTHONHASHSEED']=str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "plt.rc('figure', figsize=(20,10))\n",
    "plt.rc('axes', labelsize=18, titlesize=20, titleweight = 'bold')    # tamaño de label y titulo \n",
    "plt.rc('xtick', labelsize=14)    # tamaño de los indicadores de variacion eje x\n",
    "plt.rc('ytick', labelsize=14)    # tamaño de los indicadores de variacion eje y\n",
    "plt.rc('legend', fontsize=14)    # tamaño del indicador (por ej, verdadero o falso)\n",
    "\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR) \n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "  except RuntimeError as e:\n",
    "    print(e)\n",
    "\n",
    "%run ../0_Data/0_DataLoader.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hiperparametros y parametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "adapter_size = None #64\n",
    "learning_rate = 0.00003\n",
    "max_seq_len = 96\n",
    "batch_size = 16\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clase para tokenizar los inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tweets:\n",
    "    DATA_COLUMN = \"text\"\n",
    "    LABEL_COLUMN = \"target\"\n",
    "\n",
    "    def __init__(self, tokenizer: FullTokenizer, max_seq_len=1024, train=None, test=None):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_seq_len = 0\n",
    "        self.train = train\n",
    "        self.test = test\n",
    "        train = train[['text', 'target']].reset_index(drop=True)\n",
    "        test = test[['text', 'target']].reset_index(drop=True)\n",
    "        \n",
    "        ((self.train_x, self.train_y),\n",
    "         (self.test_x, self.test_y)) = map(self._prepare, [train, test])\n",
    "        \n",
    "        print(\"max seq_len\", self.max_seq_len)\n",
    "#         self.max_seq_len = min(self.max_seq_len, max_seq_len)\n",
    "        self.max_seq_len = max_seq_len\n",
    "        ((self.train_x, self.train_x_token_types),\n",
    "         (self.test_x, self.test_x_token_types)) = map(self._pad, [self.train_x, self.test_x])\n",
    "\n",
    "    def _prepare(self, df):\n",
    "        x, y = [], []\n",
    "        with tqdm(total=df.shape[0], unit_scale=True) as pbar:\n",
    "            for ndx, row in df.iterrows():\n",
    "                text, label = row[Tweets.DATA_COLUMN], row[Tweets.LABEL_COLUMN]\n",
    "                tokens = self.tokenizer.tokenize(text)\n",
    "                tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n",
    "                token_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
    "#                 self.max_seq_len = max(self.max_seq_len, len(token_ids))\n",
    "                x.append(token_ids)\n",
    "                y.append(int(label))\n",
    "                pbar.update()\n",
    "        return np.array(x), np.array(y)\n",
    "    \n",
    "    def _pad(self, ids):\n",
    "        x, t = [], []\n",
    "        token_type_ids = [0] * self.max_seq_len\n",
    "        for input_ids in ids:\n",
    "            input_ids = input_ids[:min(len(input_ids), self.max_seq_len - 2)]\n",
    "            input_ids = input_ids + [0] * (self.max_seq_len - len(input_ids))\n",
    "            x.append(np.array(input_ids))\n",
    "            t.append(token_type_ids)\n",
    "        return np.array(x), np.array(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funciones para compilar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_ckpt_dir=\".models/uncased_L-12_H-768_A-12/\"\n",
    "bert_ckpt_file = bert_ckpt_dir + \"bert_model.ckpt\"\n",
    "bert_config_file = bert_ckpt_dir + \"bert_config.json\"\n",
    "\n",
    "def flatten_layers(root_layer):\n",
    "    if isinstance(root_layer, tf.keras.layers.Layer):\n",
    "        yield root_layer\n",
    "    for layer in root_layer._layers:\n",
    "        for sub_layer in flatten_layers(layer):\n",
    "            yield sub_layer\n",
    "\n",
    "def freeze_bert_layers(l_bert):\n",
    "    \"\"\"\n",
    "    Freezes all but LayerNorm and adapter layers - see arXiv:1902.00751.\n",
    "    \"\"\"\n",
    "    for layer in flatten_layers(l_bert):\n",
    "        if layer.name in [\"LayerNorm\", \"adapter-down\", \"adapter-up\"]:\n",
    "            layer.trainable = True\n",
    "        elif len(layer._layers) == 0:\n",
    "            layer.trainable = False\n",
    "        l_bert.embeddings_layer.trainable = False\n",
    "\n",
    "\n",
    "def create_learning_rate_scheduler(max_learn_rate=5e-5,\n",
    "                                   end_learn_rate=1e-7,\n",
    "                                   warmup_epoch_count=10,\n",
    "                                   total_epoch_count=90):\n",
    "\n",
    "    def lr_scheduler(epoch):\n",
    "        if epoch < warmup_epoch_count:\n",
    "            res = (max_learn_rate/warmup_epoch_count) * (epoch + 1)\n",
    "        else:\n",
    "            res = max_learn_rate*math.exp(math.log(end_learn_rate/max_learn_rate)*(epoch-warmup_epoch_count+1)/(total_epoch_count-warmup_epoch_count+1))\n",
    "        return float(res)\n",
    "    learning_rate_scheduler = tf.keras.callbacks.LearningRateScheduler(lr_scheduler, verbose=1)\n",
    "\n",
    "    return learning_rate_scheduler\n",
    "\n",
    "def create_model(max_seq_len, adapter_size=64):\n",
    "  # create the bert layer\n",
    "  with tf.io.gfile.GFile(bert_config_file, \"r\") as reader:\n",
    "      bc = StockBertConfig.from_json_string(reader.read())\n",
    "      bert_params = map_stock_config_to_params(bc)\n",
    "      bert_params.adapter_size = adapter_size\n",
    "      bert = BertModelLayer.from_params(bert_params, name=\"bert\")\n",
    "        \n",
    "  input_ids      = tf.keras.layers.Input(shape=(max_seq_len,), dtype='int32', name=\"input_ids\")\n",
    "  output         = bert(input_ids)\n",
    "\n",
    "  print(\"bert shape\", output.shape)\n",
    "  cls_out = tf.keras.layers.Lambda(lambda seq: seq[:, 0, :])(output)\n",
    "#   cls_out1 = tf.keras.layers.Dropout(0.5)(cls_out)\n",
    "#   logits0 = tf.keras.layers.Dense(units=768, activation=\"tanh\")(cls_out1)\n",
    "  middle = tf.keras.layers.Dropout(0.5)(cls_out)\n",
    "  logits = tf.keras.layers.Dense(units=1, activation=None)(middle)\n",
    "\n",
    "  model = tf.keras.Model(inputs=input_ids, outputs=logits)\n",
    "  model.build(input_shape=(None, max_seq_len))\n",
    "\n",
    "  # load the pre-trained model weights\n",
    "  load_stock_weights(bert, bert_ckpt_file)\n",
    "\n",
    "  if adapter_size is not None:\n",
    "      freeze_bert_layers(bert)\n",
    "    \n",
    "    \n",
    "  radam = tfa.optimizers.RectifiedAdam(learning_rate=learning_rate)\n",
    "  ranger = tfa.optimizers.Lookahead(radam, sync_period=6, slow_step_size=0.5)\n",
    "\n",
    "    \n",
    "  model.compile(optimizer=ranger,#SGD(learning_rate=learning_rate, momentum=0.9, nesterov=True),\n",
    "                loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                metrics=[tf.keras.metrics.BinaryAccuracy()])\n",
    "\n",
    "  model.summary()\n",
    "        \n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento y validación del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4.87k/4.87k [00:03<00:00, 1.25kit/s]\n",
      "100%|██████████| 1.22k/1.22k [00:00<00:00, 1.29kit/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max seq_len 0\n",
      "bert shape (None, 96, 768)\n",
      "Done loading 196 BERT weights from: .models/uncased_L-12_H-768_A-12/bert_model.ckpt into <bert.model.BertModelLayer object at 0x000001AA6FC577C0> (prefix:bert). Count of weights not found in the checkpoint was: [0]. Count of weights with mismatched shape: [0]\n",
      "Unused weights from checkpoint: \n",
      "\tbert/embeddings/token_type_embeddings\n",
      "\tbert/pooler/dense/bias\n",
      "\tbert/pooler/dense/kernel\n",
      "\tcls/predictions/output_bias\n",
      "\tcls/predictions/transform/LayerNorm/beta\n",
      "\tcls/predictions/transform/LayerNorm/gamma\n",
      "\tcls/predictions/transform/dense/bias\n",
      "\tcls/predictions/transform/dense/kernel\n",
      "\tcls/seq_relationship/output_bias\n",
      "\tcls/seq_relationship/output_weights\n",
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_ids (InputLayer)       [(None, 96)]              0         \n",
      "_________________________________________________________________\n",
      "bert (BertModelLayer)        (None, 96, 768)           108890112 \n",
      "_________________________________________________________________\n",
      "lambda (Lambda)              (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 769       \n",
      "=================================================================\n",
      "Total params: 108,890,881\n",
      "Trainable params: 108,890,881\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\germa\\documents\\proyectos python\\7506tp2\\venv\\lib\\site-packages\\sklearn\\utils\\validation.py:68: FutureWarning: Pass classes=[0 1], y=id\n",
      "3774    0\n",
      "203     1\n",
      "4778    1\n",
      "3879    1\n",
      "4692    1\n",
      "       ..\n",
      "1607    0\n",
      "3745    1\n",
      "5002    0\n",
      "1726    0\n",
      "3933    1\n",
      "Name: target, Length: 4872, dtype: int64 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "305/305 [==============================] - 404s 1s/step - loss: 0.7691 - binary_accuracy: 0.5659 - val_loss: 0.5875 - val_binary_accuracy: 0.7282\n",
      "Epoch 2/10\n",
      "305/305 [==============================] - 402s 1s/step - loss: 0.5531 - binary_accuracy: 0.7496 - val_loss: 0.4987 - val_binary_accuracy: 0.7997\n",
      "Epoch 3/10\n",
      "305/305 [==============================] - 394s 1s/step - loss: 0.4254 - binary_accuracy: 0.8292 - val_loss: 0.4255 - val_binary_accuracy: 0.8186\n",
      "Epoch 4/10\n",
      "305/305 [==============================] - 413s 1s/step - loss: 0.3700 - binary_accuracy: 0.8493 - val_loss: 0.4220 - val_binary_accuracy: 0.8095\n",
      "Epoch 5/10\n",
      "305/305 [==============================] - 420s 1s/step - loss: 0.3277 - binary_accuracy: 0.8771 - val_loss: 0.4343 - val_binary_accuracy: 0.8177\n",
      "Epoch 6/10\n",
      "305/305 [==============================] - 406s 1s/step - loss: 0.2719 - binary_accuracy: 0.8994 - val_loss: 0.5238 - val_binary_accuracy: 0.7947\n",
      "153/153 [==============================] - 49s 322ms/step - loss: 0.2639 - binary_accuracy: 0.8980\n",
      "39/39 [==============================] - 12s 318ms/step - loss: 0.4220 - binary_accuracy: 0.8095\n",
      "train acc 0.897988498210907\n",
      " test acc 0.8095238208770752\n",
      "results [0.8095238208770752]\n",
      "Mean-Precision: 0.8095238208770752\n",
      "Wall time: 43min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "tokenizer = FullTokenizer(vocab_file=os.path.join(bert_ckpt_dir, \"vocab.txt\"))\n",
    "\n",
    "results = []\n",
    "history = 0\n",
    "def entrenar(train_df, val_df):\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "    tf.compat.v1.reset_default_graph()\n",
    "    gc.collect()\n",
    "\n",
    "    global max_seq_len\n",
    "    \n",
    "    data = Tweets(tokenizer, max_seq_len=max_seq_len, train=train_df, test=val_df)    \n",
    "    \n",
    "    model = create_model(max_seq_len, adapter_size=adapter_size)\n",
    "        \n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logs_base_dir)\n",
    "\n",
    "    class_weights = compute_class_weight('balanced', np.unique(train_df.target), train_df.target) \n",
    "    class_weights_dict = dict(enumerate(class_weights))\n",
    "    \n",
    "    history = model.fit(x=data.train_x, y=data.train_y, batch_size=batch_size, epochs=epochs,\n",
    "                validation_data=(data.test_x, data.test_y),\n",
    "                class_weight = class_weights_dict,                        \n",
    "                callbacks=[tensorboard_callback, \n",
    "                           tf.keras.callbacks.EarlyStopping(patience=2, restore_best_weights=True)])\n",
    "\n",
    "#               callbacks=[create_learning_rate_scheduler(max_learn_rate=5e-5,\n",
    "#                                                         end_learn_rate=1e-5,\n",
    "#                                                         warmup_epoch_count=1,\n",
    "#                                                         total_epoch_count=epochs),\n",
    "#                          tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True),\n",
    "#                          tensorboard_callback])\n",
    "    \n",
    "    _, train_acc = model.evaluate(data.train_x, data.train_y)\n",
    "    _, test_acc = model.evaluate(data.test_x , data.test_y)\n",
    "    print(\"train acc\", train_acc)\n",
    "    print(\" test acc\", test_acc)\n",
    "    results.append(test_acc)\n",
    "    model.save_weights('./real_or_not.h5', overwrite=True)\n",
    "    tf.keras.backend.clear_session()\n",
    "    tf.compat.v1.reset_default_graph()\n",
    "    gc.collect()\n",
    "\n",
    "datos = get_data_original_as_database()\n",
    "\n",
    "test_data = datos.test\n",
    "train_df = datos.train\n",
    "val_data = datos.validation\n",
    "predict = datos.predict\n",
    "\n",
    "entrenar(train_df, val_data)\n",
    "\n",
    "# split, df_train, test_data = get_k_folded_data_original_as_database()\n",
    "\n",
    "# #Mantenemos los pesos de cada clase en cada K-Fold\n",
    "# for train_index, val_index in split:\n",
    "#     train_df = df_train.iloc[train_index]\n",
    "#     val_df = df_train.iloc[val_index]\n",
    "#     entrenar(train_df, val_df)\n",
    "\n",
    "    \n",
    "print(\"results\",results)\n",
    "print(f\"Mean-Precision: {sum(results) / len(results)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testeo del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4.87k/4.87k [00:02<00:00, 1.68kit/s]\n",
      "100%|██████████| 1.52k/1.52k [00:00<00:00, 1.66kit/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max seq_len 0\n",
      "bert shape (None, 96, 768)\n",
      "Done loading 196 BERT weights from: .models/uncased_L-12_H-768_A-12/bert_model.ckpt into <bert.model.BertModelLayer object at 0x000001AA4AD4EFD0> (prefix:bert). Count of weights not found in the checkpoint was: [0]. Count of weights with mismatched shape: [0]\n",
      "Unused weights from checkpoint: \n",
      "\tbert/embeddings/token_type_embeddings\n",
      "\tbert/pooler/dense/bias\n",
      "\tbert/pooler/dense/kernel\n",
      "\tcls/predictions/output_bias\n",
      "\tcls/predictions/transform/LayerNorm/beta\n",
      "\tcls/predictions/transform/LayerNorm/gamma\n",
      "\tcls/predictions/transform/dense/bias\n",
      "\tcls/predictions/transform/dense/kernel\n",
      "\tcls/seq_relationship/output_bias\n",
      "\tcls/seq_relationship/output_weights\n",
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_ids (InputLayer)       [(None, 96)]              0         \n",
      "_________________________________________________________________\n",
      "bert (BertModelLayer)        (None, 96, 768)           108890112 \n",
      "_________________________________________________________________\n",
      "lambda (Lambda)              (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 769       \n",
      "=================================================================\n",
      "Total params: 108,890,881\n",
      "Trainable params: 108,890,881\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "48/48 [==============================] - 15s 320ms/step - loss: 0.3802 - binary_accuracy: 0.8372\n",
      "train acc 0.837163507938385\n",
      "Wall time: 31 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "tokenizer = FullTokenizer(vocab_file=os.path.join(bert_ckpt_dir, \"vocab.txt\"))\n",
    "\n",
    "data = Tweets(tokenizer, max_seq_len=max_seq_len, train=train_df, test=test_data)    \n",
    "model = create_model(max_seq_len, adapter_size=adapter_size)\n",
    "model.load_weights(\"real_or_not.h5\")\n",
    "\n",
    "\n",
    "_, test_acc = model.evaluate(data.test_x, data.test_y)\n",
    "\n",
    "print(\"train acc\", test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predecimos los datos y submiteamos a Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert shape (None, 96, 768)\n",
      "Done loading 196 BERT weights from: .models/uncased_L-12_H-768_A-12/bert_model.ckpt into <bert.model.BertModelLayer object at 0x000002BEA1E90880> (prefix:bert). Count of weights not found in the checkpoint was: [0]. Count of weights with mismatched shape: [0]\n",
      "Unused weights from checkpoint: \n",
      "\tbert/embeddings/token_type_embeddings\n",
      "\tbert/pooler/dense/bias\n",
      "\tbert/pooler/dense/kernel\n",
      "\tcls/predictions/output_bias\n",
      "\tcls/predictions/transform/LayerNorm/beta\n",
      "\tcls/predictions/transform/LayerNorm/gamma\n",
      "\tcls/predictions/transform/dense/bias\n",
      "\tcls/predictions/transform/dense/kernel\n",
      "\tcls/seq_relationship/output_bias\n",
      "\tcls/seq_relationship/output_weights\n",
      "Model: \"functional_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_ids (InputLayer)       [(None, 96)]              0         \n",
      "_________________________________________________________________\n",
      "bert (BertModelLayer)        (None, 96, 768)           108890112 \n",
      "_________________________________________________________________\n",
      "lambda_1 (Lambda)            (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 769       \n",
      "=================================================================\n",
      "Total params: 108,890,881\n",
      "Trainable params: 108,890,881\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "204/204 [==============================] - 37s 180ms/step\n",
      "Successfully submitted to Real or Not? NLP with Disaster Tweets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0.00/25.4k [00:00<?, ?B/s]\n",
      " 31%|###1      | 8.00k/25.4k [00:00<00:00, 56.4kB/s]\n",
      "100%|##########| 25.4k/25.4k [00:02<00:00, 9.77kB/s]\n"
     ]
    }
   ],
   "source": [
    "x, y = [], []\n",
    "for ndx, row in predict.iterrows():\n",
    "    text = row['text']\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    x.append(token_ids)\n",
    "ids = np.array(x)\n",
    "\n",
    "x, t = [], []\n",
    "token_type_ids = [0] * max_seq_len\n",
    "for input_ids in ids:\n",
    "    input_ids = input_ids[:min(len(input_ids), max_seq_len - 2)]\n",
    "    input_ids = input_ids + [0] * (max_seq_len - len(input_ids))\n",
    "    x.append(np.array(input_ids))\n",
    "    t.append(token_type_ids)\n",
    "test_x, test_x_token_types = np.array(x), np.array(t)\n",
    "\n",
    "model = create_model(max_seq_len, adapter_size=adapter_size)\n",
    "model.load_weights(\"real_or_not.h5\")\n",
    "\n",
    "y_test = (model.predict(test_x, batch_size=batch_size, verbose=1) > 0.5).astype(\"int32\")\n",
    "submission = pd.read_csv('../dataset/sample_submission.csv')\n",
    "submission['target'] = y_test\n",
    "submission.to_csv(\"submission3.csv\", index=False)\n",
    "\n",
    "!kaggle competitions submit nlp-getting-started -f submission3.csv -m 'BERT'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}