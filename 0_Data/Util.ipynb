{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\germa\\documents\\proyectos python\\tp1\\venv\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\users\\germa\\documents\\proyectos python\\tp1\\venv\\lib\\site-packages\\numpy\\.libs\\libopenblas.NOIJJG62EMASZI6NYURL6JBKM4EVBGM7.gfortran-win_amd64.dll\n",
      "c:\\users\\germa\\documents\\proyectos python\\tp1\\venv\\lib\\site-packages\\numpy\\.libs\\libopenblas.PYQHXLVVQ7VESDPUVUADXEVJOBGHJPAY.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\\n%s\" %\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\germa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\germa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\germa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Paquetes para Preprocesamiento\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import stopwords \n",
    "from nltk import ngrams\n",
    "from nltk import regexp_tokenize          \n",
    "import unicodedata\n",
    "import demoji\n",
    "from nltk.stem import SnowballStemmer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_sentiment_analysis(texto):\n",
    "    texto = remover_url(texto)\n",
    "    texto = remover_usuario(texto)\n",
    "    texto = demoji.replace(texto) #transformamos los emojis en palabras\n",
    "    texto = remover_tag_html(texto)\n",
    "    texto = remover_espacio_extra(texto)\n",
    "    texto = remover_punto(texto)\n",
    "    texto = stemmizar_expandir_abreviaturas_contracciones(texto)\n",
    "    return texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpiar_texto(texto):\n",
    "    texto = minuscula(texto)\n",
    "    texto = remover_url(texto)\n",
    "    texto = remover_usuario(texto)\n",
    "    texto = remover_emoji(texto)\n",
    "    texto = remover_caracter_especial(texto)\n",
    "    texto = remover_tag_html(texto)\n",
    "    texto = remover_acento(texto)\n",
    "    texto = remover_punto_numero(texto)\n",
    "    texto = remover_espacio_extra(texto)\n",
    "    texto = lematizar_remover_stop_words_abreviaturas_contracciones(texto)\n",
    "    return texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_simple(texto):\n",
    "    texto = minuscula(texto)\n",
    "    texto = remover_url(texto)\n",
    "    texto = remover_usuario(texto)\n",
    "    texto = demoji.replace(texto) #transformamos los emojis en palabras\n",
    "    texto = remover_tag_html(texto)\n",
    "    texto = remover_espacio_extra(texto)\n",
    "    texto = expandir_abreviatura(texto)\n",
    "    texto = expandir_contraccion(texto)\n",
    "    texto = remover_punto(texto)\n",
    "    return texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Transformar el texto a minuscula\n",
    "\n",
    "def minuscula(texto):\n",
    "\n",
    "    return texto.lower()\n",
    "\n",
    "#Remover URL\n",
    "\n",
    "def remover_url(texto):\n",
    "\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "\n",
    "    return url.sub(r'', texto)\n",
    "\n",
    "\n",
    "#Remover hashtags que aparecen (#hashtag)\n",
    "def remover_hashtags(texto):\n",
    "    return re.sub(\"#[A-Za-z0-9]+\",\"\", texto)\n",
    "\n",
    "\n",
    "#Remover Usuarios que aparecen (@usuario)\n",
    "\n",
    "def remover_usuario(text):\n",
    "\n",
    "    text = re.sub(r\"\\@[A-Za-z0-9]+\", \"\", text)\n",
    "\n",
    "    return text\n",
    "\n",
    "# Remover Emoji\n",
    "\n",
    "def remover_emoji(texto):\n",
    "\n",
    "    emoji_patrones = re.compile(\n",
    "\n",
    "        '['\n",
    "\n",
    "        u'\\U0001F600-\\U0001F64F' \n",
    "\n",
    "        u'\\U0001F300-\\U0001F5FF' \n",
    "\n",
    "        u'\\U0001F680-\\U0001F6FF' \n",
    "\n",
    "        u'\\U0001F1E0-\\U0001F1FF' \n",
    "\n",
    "        u'\\U00002702-\\U000027B0'\n",
    "\n",
    "        u'\\U000024C2-\\U0001F251'\n",
    "\n",
    "        ']+',\n",
    "\n",
    "        flags=re.UNICODE)\n",
    "\n",
    "    return emoji_patrones.sub(r'', texto)\n",
    "\n",
    "#Expandir las Abreviaturas\n",
    "\n",
    "abreviaturas = {\n",
    "\n",
    "    \"$\" : \" dollar \",\n",
    "\n",
    "    \"â‚¬\" : \" euro \",\n",
    "\n",
    "    \"4ao\" : \"for adults only\",\n",
    "\n",
    "    \"a.m\" : \"before midday\",\n",
    "\n",
    "    \"a3\" : \"anytime anywhere anyplace\",\n",
    "\n",
    "    \"aamof\" : \"as a matter of fact\",\n",
    "\n",
    "    \"acct\" : \"account\",\n",
    "\n",
    "    \"adih\" : \"another day in hell\",\n",
    "\n",
    "    \"afaic\" : \"as far as i am concerned\",\n",
    "\n",
    "    \"afaict\" : \"as far as i can tell\",\n",
    "\n",
    "    \"afaik\" : \"as far as i know\",\n",
    "\n",
    "    \"afair\" : \"as far as i remember\",\n",
    "\n",
    "    \"afk\" : \"away from keyboard\",\n",
    "\n",
    "    \"app\" : \"application\",\n",
    "\n",
    "    \"approx\" : \"approximately\",\n",
    "\n",
    "    \"apps\" : \"applications\",\n",
    "\n",
    "    \"asap\" : \"as soon as possible\",\n",
    "\n",
    "    \"asl\" : \"age, sex, location\",\n",
    "\n",
    "    \"atk\" : \"at the keyboard\",\n",
    "\n",
    "    \"ave.\" : \"avenue\",\n",
    "\n",
    "    \"aymm\" : \"are you my mother\",\n",
    "\n",
    "    \"ayor\" : \"at your own risk\", \n",
    "\n",
    "    \"b&b\" : \"bed and breakfast\",\n",
    "\n",
    "    \"b+b\" : \"bed and breakfast\",\n",
    "\n",
    "    \"b.c\" : \"before christ\",\n",
    "\n",
    "    \"b2b\" : \"business to business\",\n",
    "\n",
    "    \"b2c\" : \"business to customer\",\n",
    "\n",
    "    \"b4\" : \"before\",\n",
    "\n",
    "    \"b4n\" : \"bye for now\",\n",
    "\n",
    "    \"b@u\" : \"back at you\",\n",
    "\n",
    "    \"bae\" : \"before anyone else\",\n",
    "\n",
    "    \"bak\" : \"back at keyboard\",\n",
    "\n",
    "    \"bbbg\" : \"bye bye be good\",\n",
    "\n",
    "    \"bbc\" : \"british broadcasting corporation\",\n",
    "\n",
    "    \"bbias\" : \"be back in a second\",\n",
    "\n",
    "    \"bbl\" : \"be back later\",\n",
    "\n",
    "    \"bbs\" : \"be back soon\",\n",
    "\n",
    "    \"be4\" : \"before\",\n",
    "\n",
    "    \"bfn\" : \"bye for now\",\n",
    "\n",
    "    \"blvd\" : \"boulevard\",\n",
    "\n",
    "    \"bout\" : \"about\",\n",
    "\n",
    "    \"brb\" : \"be right back\",\n",
    "\n",
    "    \"bros\" : \"brothers\",\n",
    "\n",
    "    \"brt\" : \"be right there\",\n",
    "\n",
    "    \"bsaaw\" : \"big smile and a wink\",\n",
    "\n",
    "    \"btw\" : \"by the way\",\n",
    "\n",
    "    \"bwl\" : \"bursting with laughter\",\n",
    "\n",
    "    \"c/o\" : \"care of\",\n",
    "\n",
    "    \"cet\" : \"central european time\",\n",
    "\n",
    "    \"cf\" : \"compare\",\n",
    "\n",
    "    \"cia\" : \"central intelligence agency\",\n",
    "\n",
    "    \"csl\" : \"can not stop laughing\",\n",
    "\n",
    "    \"cu\" : \"see you\",\n",
    "\n",
    "    \"cul8r\" : \"see you later\",\n",
    "\n",
    "    \"cv\" : \"curriculum vitae\",\n",
    "\n",
    "    \"cwot\" : \"complete waste of time\",\n",
    "\n",
    "    \"cya\" : \"see you\",\n",
    "\n",
    "    \"cyt\" : \"see you tomorrow\",\n",
    "\n",
    "    \"dae\" : \"does anyone else\",\n",
    "\n",
    "    \"dbmib\" : \"do not bother me i am busy\",\n",
    "\n",
    "    \"diy\" : \"do it yourself\",\n",
    "\n",
    "    \"dm\" : \"direct message\",\n",
    "\n",
    "    \"dwh\" : \"during work hours\",\n",
    "\n",
    "    \"e123\" : \"easy as one two three\",\n",
    "\n",
    "    \"eet\" : \"eastern european time\",\n",
    "\n",
    "    \"eg\" : \"example\",\n",
    "\n",
    "    \"embm\" : \"early morning business meeting\",\n",
    "\n",
    "    \"encl\" : \"enclosed\",\n",
    "\n",
    "    \"encl.\" : \"enclosed\",\n",
    "\n",
    "    \"etc\" : \"and so on\",\n",
    "\n",
    "    \"faq\" : \"frequently asked questions\",\n",
    "\n",
    "    \"fawc\" : \"for anyone who cares\",\n",
    "\n",
    "    \"fb\" : \"facebook\",\n",
    "\n",
    "    \"fc\" : \"fingers crossed\",\n",
    "\n",
    "    \"fig\" : \"figure\",\n",
    "\n",
    "    \"fimh\" : \"forever in my heart\", \n",
    "\n",
    "    \"ft.\" : \"feet\",\n",
    "\n",
    "    \"ft\" : \"featuring\",\n",
    "\n",
    "    \"ftl\" : \"for the loss\",\n",
    "\n",
    "    \"ftw\" : \"for the win\",\n",
    "\n",
    "    \"fwiw\" : \"for what it is worth\",\n",
    "\n",
    "    \"fyi\" : \"for your information\",\n",
    "\n",
    "    \"g9\" : \"genius\",\n",
    "\n",
    "    \"gahoy\" : \"get a hold of yourself\",\n",
    "\n",
    "    \"gal\" : \"get a life\",\n",
    "\n",
    "    \"gcse\" : \"general certificate of secondary education\",\n",
    "\n",
    "    \"gfn\" : \"gone for now\",\n",
    "\n",
    "    \"gg\" : \"good game\",\n",
    "\n",
    "    \"gl\" : \"good luck\",\n",
    "\n",
    "    \"glhf\" : \"good luck have fun\",\n",
    "\n",
    "    \"gmt\" : \"greenwich mean time\",\n",
    "\n",
    "    \"gmta\" : \"great minds think alike\",\n",
    "\n",
    "    \"gn\" : \"good night\",\n",
    "\n",
    "    \"g.o.a.t\" : \"greatest of all time\",\n",
    "\n",
    "    \"goat\" : \"greatest of all time\",\n",
    "\n",
    "    \"goi\" : \"get over it\",\n",
    "\n",
    "    \"gps\" : \"global positioning system\",\n",
    "\n",
    "    \"gr8\" : \"great\",\n",
    "\n",
    "    \"gratz\" : \"congratulations\",\n",
    "\n",
    "    \"gyal\" : \"girl\",\n",
    "\n",
    "    \"h&c\" : \"hot and cold\",\n",
    "\n",
    "    \"hp\" : \"horsepower\",\n",
    "\n",
    "    \"hr\" : \"hour\",\n",
    "\n",
    "    \"hrh\" : \"his royal highness\",\n",
    "\n",
    "    \"ht\" : \"height\",\n",
    "\n",
    "    \"ibrb\" : \"i will be right back\",\n",
    "\n",
    "    \"ic\" : \"i see\",\n",
    "\n",
    "    \"icq\" : \"i seek you\",\n",
    "\n",
    "    \"icymi\" : \"in case you missed it\",\n",
    "\n",
    "    \"idc\" : \"i do not care\",\n",
    "\n",
    "    \"idgadf\" : \"i do not give a damn fuck\",\n",
    "\n",
    "    \"idgaf\" : \"i do not give a fuck\",\n",
    "\n",
    "    \"idk\" : \"i do not know\",\n",
    "\n",
    "    \"ie\" : \"that is\",\n",
    "\n",
    "    \"i.e\" : \"that is\",\n",
    "\n",
    "    \"ifyp\" : \"i feel your pain\",\n",
    "\n",
    "    \"IG\" : \"instagram\",\n",
    "\n",
    "    \"iirc\" : \"if i remember correctly\",\n",
    "\n",
    "    \"ilu\" : \"i love you\",\n",
    "\n",
    "    \"ily\" : \"i love you\",\n",
    "\n",
    "    \"imho\" : \"in my humble opinion\",\n",
    "\n",
    "    \"imo\" : \"in my opinion\",\n",
    "\n",
    "    \"imu\" : \"i miss you\",\n",
    "\n",
    "    \"iow\" : \"in other words\",\n",
    "\n",
    "    \"irl\" : \"in real life\",\n",
    "\n",
    "    \"j4f\" : \"just for fun\",\n",
    "\n",
    "    \"jic\" : \"just in case\",\n",
    "\n",
    "    \"jk\" : \"just kidding\",\n",
    "\n",
    "    \"jsyk\" : \"just so you know\",\n",
    "\n",
    "    \"l8r\" : \"later\",\n",
    "\n",
    "    \"lb\" : \"pound\",\n",
    "\n",
    "    \"lbs\" : \"pounds\",\n",
    "\n",
    "    \"ldr\" : \"long distance relationship\",\n",
    "\n",
    "    \"lmao\" : \"laugh my ass off\",\n",
    "\n",
    "    \"lmfao\" : \"laugh my fucking ass off\",\n",
    "\n",
    "    \"lol\" : \"laughing out loud\",\n",
    "\n",
    "    \"ltd\" : \"limited\",\n",
    "\n",
    "    \"ltns\" : \"long time no see\",\n",
    "\n",
    "    \"m8\" : \"mate\",\n",
    "\n",
    "    \"mf\" : \"motherfucker\",\n",
    "\n",
    "    \"mfs\" : \"motherfuckers\",\n",
    "\n",
    "    \"mfw\" : \"my face when\",\n",
    "\n",
    "    \"mofo\" : \"motherfucker\",\n",
    "\n",
    "    \"mph\" : \"miles per hour\",\n",
    "\n",
    "    \"mr\" : \"mister\",\n",
    "\n",
    "    \"mrw\" : \"my reaction when\",\n",
    "\n",
    "    \"ms\" : \"miss\",\n",
    "\n",
    "    \"mte\" : \"my thoughts exactly\",\n",
    "\n",
    "    \"nagi\" : \"not a good idea\",\n",
    "\n",
    "    \"nbc\" : \"national broadcasting company\",\n",
    "\n",
    "    \"nbd\" : \"not big deal\",\n",
    "\n",
    "    \"nfs\" : \"not for sale\",\n",
    "\n",
    "    \"ngl\" : \"not going to lie\",\n",
    "\n",
    "    \"nhs\" : \"national health service\",\n",
    "\n",
    "    \"nrn\" : \"no reply necessary\",\n",
    "\n",
    "    \"nsfl\" : \"not safe for life\",\n",
    "\n",
    "    \"nsfw\" : \"not safe for work\",\n",
    "\n",
    "    \"nth\" : \"nice to have\",\n",
    "\n",
    "    \"nvr\" : \"never\",\n",
    "\n",
    "    \"nyc\" : \"new york city\",\n",
    "\n",
    "    \"oc\" : \"original content\",\n",
    "\n",
    "    \"og\" : \"original\",\n",
    "\n",
    "    \"ohp\" : \"overhead projector\",\n",
    "\n",
    "    \"oic\" : \"oh i see\",\n",
    "\n",
    "    \"omdb\" : \"over my dead body\",\n",
    "\n",
    "    \"omg\" : \"oh my god\",\n",
    "\n",
    "    \"omw\" : \"on my way\",\n",
    "\n",
    "    \"p.a\" : \"per annum\",\n",
    "\n",
    "    \"p.m\" : \"after midday\",\n",
    "\n",
    "    \"pm\" : \"prime minister\",\n",
    "\n",
    "    \"poc\" : \"people of color\",\n",
    "\n",
    "    \"pov\" : \"point of view\",\n",
    "\n",
    "    \"pp\" : \"pages\",\n",
    "\n",
    "    \"ppl\" : \"people\",\n",
    "\n",
    "    \"prw\" : \"parents are watching\",\n",
    "\n",
    "    \"ps\" : \"postscript\",\n",
    "\n",
    "    \"pt\" : \"point\",\n",
    "\n",
    "    \"ptb\" : \"please text back\",\n",
    "\n",
    "    \"pto\" : \"please turn over\",\n",
    "\n",
    "    \"qpsa\" : \"what happens\", \n",
    "\n",
    "    \"ratchet\" : \"rude\",\n",
    "\n",
    "    \"rbtl\" : \"read between the lines\",\n",
    "\n",
    "    \"rlrt\" : \"real life retweet\", \n",
    "\n",
    "    \"rofl\" : \"rolling on the floor laughing\",\n",
    "\n",
    "    \"roflol\" : \"rolling on the floor laughing out loud\",\n",
    "\n",
    "    \"rotflmao\" : \"rolling on the floor laughing my ass off\",\n",
    "\n",
    "    \"rt\" : \"retweet\",\n",
    "\n",
    "    \"ruok\" : \"are you ok\",\n",
    "\n",
    "    \"sfw\" : \"safe for work\",\n",
    "\n",
    "    \"sk8\" : \"skate\",\n",
    "\n",
    "    \"smh\" : \"shake my head\",\n",
    "\n",
    "    \"sq\" : \"square\",\n",
    "\n",
    "    \"srsly\" : \"seriously\", \n",
    "\n",
    "    \"ssdd\" : \"same stuff different day\",\n",
    "\n",
    "    \"tbh\" : \"to be honest\",\n",
    "\n",
    "    \"tbs\" : \"tablespooful\",\n",
    "\n",
    "    \"tbsp\" : \"tablespooful\",\n",
    "\n",
    "    \"tfw\" : \"that feeling when\",\n",
    "\n",
    "    \"thks\" : \"thank you\",\n",
    "\n",
    "    \"tho\" : \"though\",\n",
    "\n",
    "    \"thx\" : \"thank you\",\n",
    "\n",
    "    \"tia\" : \"thanks in advance\",\n",
    "\n",
    "    \"til\" : \"today i learned\",\n",
    "\n",
    "    \"tl;dr\" : \"too long i did not read\",\n",
    "\n",
    "    \"tldr\" : \"too long i did not read\",\n",
    "\n",
    "    \"tmb\" : \"tweet me back\",\n",
    "\n",
    "    \"tntl\" : \"trying not to laugh\",\n",
    "\n",
    "    \"ttyl\" : \"talk to you later\",\n",
    "\n",
    "    \"u\" : \"you\",\n",
    "\n",
    "    \"u2\" : \"you too\",\n",
    "\n",
    "    \"u4e\" : \"yours for ever\",\n",
    "\n",
    "    \"utc\" : \"coordinated universal time\",\n",
    "\n",
    "    \"w/\" : \"with\",\n",
    "\n",
    "    \"w/o\" : \"without\",\n",
    "\n",
    "    \"w8\" : \"wait\",\n",
    "\n",
    "    \"wassup\" : \"what is up\",\n",
    "\n",
    "    \"wb\" : \"welcome back\",\n",
    "\n",
    "    \"wtf\" : \"what the fuck\",\n",
    "\n",
    "    \"wtg\" : \"way to go\",\n",
    "\n",
    "    \"wtpa\" : \"where the party at\",\n",
    "\n",
    "    \"wuf\" : \"where are you from\",\n",
    "\n",
    "    \"wuzup\" : \"what is up\",\n",
    "\n",
    "    \"wywh\" : \"wish you were here\",\n",
    "\n",
    "    \"yd\" : \"yard\",\n",
    "\n",
    "    \"ygtr\" : \"you got that right\",\n",
    "\n",
    "    \"ynk\" : \"you never know\",\n",
    "\n",
    "    \"zzz\" : \"sleeping bored and tired\"\n",
    "\n",
    "}\n",
    "\n",
    "def expandir_abreviatura(texto,mapping = abreviaturas):\n",
    "\n",
    "    texto = ' '.join([mapping[t] if t in mapping else t for t in texto.split(\" \")])\n",
    "\n",
    "    return texto\n",
    "\n",
    "#Expandir las Contracciones\n",
    "\n",
    "contracciones_mapeo = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \n",
    "\n",
    "                       \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \n",
    "\n",
    "                       \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \n",
    "\n",
    "                       \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \n",
    "\n",
    "                       \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\n",
    "\n",
    "                       \"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \n",
    "\n",
    "                       \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \n",
    "\n",
    "                       \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\",\n",
    "\n",
    "                       \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \n",
    "\n",
    "                       \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \n",
    "\n",
    "                       \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \n",
    "\n",
    "                       \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \n",
    "\n",
    "                       \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \n",
    "\n",
    "                       \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \n",
    "\n",
    "                       \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \n",
    "\n",
    "                       \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \n",
    "\n",
    "                       \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" }\n",
    "\n",
    "\n",
    "def expandir_contraccion(texto,mapping = contracciones_mapeo):\n",
    "\n",
    "    specials =[\"â€™\", \"â€˜\", \"Â´\", \"`\"]\n",
    "\n",
    "    for s in specials:\n",
    "\n",
    "        texto = texto.replace(s,\"'\")\n",
    "\n",
    "    \n",
    "\n",
    "    texto = ' '.join([mapping[t] if t in mapping else t for t in texto.split(\" \")])\n",
    "\n",
    "    return texto\n",
    "\n",
    "#Remover Caracteres Especiales\n",
    "\n",
    "def remover_caracter_especial(texto):\n",
    "\n",
    "    pat = r'[^a-zA-z0-9.,!?/:;\\\"\\'\\s]' \n",
    "\n",
    "    return re.sub(pat, '', texto)\n",
    "\n",
    "# Remover los tags HTML\n",
    "\n",
    "def remover_tag_html(texto):\n",
    "\n",
    "    html = re.compile(r'<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n",
    "\n",
    "    return re.sub(html, '', texto)\n",
    "\n",
    "#Remover acentos\n",
    "\n",
    "def remover_acento(texto):\n",
    "\n",
    "    texto = unicodedata.normalize('NFKD', texto).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "\n",
    "    return texto\n",
    "\n",
    "#Remover Puntos\n",
    "\n",
    "def remover_punto(texto):\n",
    "    \n",
    "    return texto.translate(str.maketrans('','',string.punctuation))\n",
    "\n",
    "    #texto = ''.join([c for c in texto if c not in string.punctuation])\n",
    "\n",
    "    #return texto\n",
    "\n",
    "#Remover Numeros\n",
    "\n",
    "def remover_numero(texto):\n",
    "\n",
    "    #texto = ''.join([i for i in texto if not i.isdigit()])\n",
    "\n",
    "    #return texto\n",
    "    \n",
    "    return texto.translate(str.maketrans('','',string.digits))\n",
    "\n",
    "def remover_punto_numero(texto):\n",
    "\n",
    "    import string\n",
    "\n",
    "    texto = ''.join([c for c in texto if c not in string.punctuation and not c.isdigit()])\n",
    "\n",
    "    return texto\n",
    "\n",
    "#Remover espacios en Blanco (extras/tabs)\n",
    "\n",
    "def remover_espacio_extra(texto):\n",
    "\n",
    "    pattern = r'^\\s*|\\s\\s*'\n",
    "\n",
    "    return re.sub(pattern, ' ', texto).strip()\n",
    "\n",
    "#Remover Stop-Word\n",
    "\n",
    "def remover_stop_word(texto):\n",
    "\n",
    "    return \" \".join ([word for word in word_tokenize(texto) if not word in stopwords.words('english')])\n",
    "\n",
    "#Lematizar\n",
    "\n",
    "def lematizar(texto):\n",
    "    lemma = WordNetLemmatizer()\n",
    "    return \" \".join([lemma.lemmatize(word) for word in word_tokenize(texto)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Usamos esta version del tokenizador por la ventaja en performance que tiene\n",
    "lemma = WordNetLemmatizer()\n",
    "#cacheamos los stopwords porque la diferencia en performance es ENORME\n",
    "cachedStopWords = stopwords.words(\"english\")\n",
    "specials =[\"â€™\", \"â€˜\", \"Â´\", \"`\"]\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def stemmizar_expandir_abreviaturas_contracciones(texto, mapping = contracciones_mapeo, mapping_abreviaturas = abreviaturas):\n",
    "\n",
    "    tokenizacion = regexp_tokenize(texto, '\\s', gaps=True)\n",
    "\n",
    "    for s in specials:\n",
    "\n",
    "        texto = texto.replace(s,\"'\")\n",
    "    \n",
    "    return \" \".join([stemmer.stem(mapping_abreviaturas[word] if word in mapping_abreviaturas else mapping[word] if word in mapping else word) for word in tokenizacion])\n",
    "\n",
    "\n",
    "#Lematizar y remover Stop-Words\n",
    "def lematizar_remover_stop_words_abreviaturas_contracciones(texto, mapping = contracciones_mapeo, mapping_abreviaturas = abreviaturas):\n",
    "\n",
    "    tokenizacion = regexp_tokenize(texto, '\\s', gaps=True)\n",
    "\n",
    "    for s in specials:\n",
    "\n",
    "        texto = texto.replace(s,\"'\")\n",
    "    \n",
    "    return \" \".join([lemma.lemmatize(mapping_abreviaturas[word] if word in mapping_abreviaturas else mapping[word] if word in mapping else word) for word in tokenizacion if word not in cachedStopWords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Usamos esta version del tokenizador por la ventaja en performance que tiene\n",
    "lemma = WordNetLemmatizer()\n",
    "#cacheamos los stopwords porque la diferencia en performance es ENORME\n",
    "cachedStopWords = stopwords.words(\"english\")\n",
    "specials =[\"â€™\", \"â€˜\", \"Â´\", \"`\"]\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def stemmizar_expandir_abreviaturas_contracciones(texto, mapping = contracciones_mapeo, mapping_abreviaturas = abreviaturas):\n",
    "\n",
    "    tokenizacion = regexp_tokenize(texto, '\\s', gaps=True)\n",
    "\n",
    "    for s in specials:\n",
    "\n",
    "        texto = texto.replace(s,\"'\")\n",
    "    \n",
    "    return \" \".join([stemmer.stem(mapping_abreviaturas[word] if word in mapping_abreviaturas else mapping[word] if word in mapping else word) for word in tokenizacion])\n",
    "\n",
    "\n",
    "#Lematizar y remover Stop-Words\n",
    "def lematizar_remover_stop_words_abreviaturas_contracciones(texto, mapping = contracciones_mapeo, mapping_abreviaturas = abreviaturas):\n",
    "\n",
    "    tokenizacion = regexp_tokenize(texto, '\\s', gaps=True)\n",
    "\n",
    "    for s in specials:\n",
    "\n",
    "        texto = texto.replace(s,\"'\")\n",
    "    \n",
    "    return \" \".join([lemma.lemmatize(mapping_abreviaturas[word] if word in mapping_abreviaturas else mapping[word] if word in mapping else word) for word in tokenizacion if word not in cachedStopWords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Usamos esta version del tokenizador por la ventaja en performance que tiene\n",
    "lemma = WordNetLemmatizer()\n",
    "#cacheamos los stopwords porque la diferencia en performance es ENORME\n",
    "cachedStopWords = stopwords.words(\"english\")\n",
    "specials =[\"â€™\", \"â€˜\", \"Â´\", \"`\"]\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def stemmizar_expandir_abreviaturas_contracciones(texto, mapping = contracciones_mapeo, mapping_abreviaturas = abreviaturas):\n",
    "\n",
    "    tokenizacion = regexp_tokenize(texto, '\\s', gaps=True)\n",
    "\n",
    "    for s in specials:\n",
    "\n",
    "        texto = texto.replace(s,\"'\")\n",
    "    \n",
    "    return \" \".join([stemmer.stem(mapping_abreviaturas[word] if word in mapping_abreviaturas else mapping[word] if word in mapping else word) for word in tokenizacion])\n",
    "\n",
    "\n",
    "#Lematizar y remover Stop-Words\n",
    "def lematizar_remover_stop_words_abreviaturas_contracciones(texto, mapping = contracciones_mapeo, mapping_abreviaturas = abreviaturas):\n",
    "\n",
    "    tokenizacion = regexp_tokenize(texto, '\\s', gaps=True)\n",
    "\n",
    "    for s in specials:\n",
    "\n",
    "        texto = texto.replace(s,\"'\")\n",
    "    \n",
    "    return \" \".join([lemma.lemmatize(mapping_abreviaturas[word] if word in mapping_abreviaturas else mapping[word] if word in mapping else word) for word in tokenizacion if word not in cachedStopWords])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
